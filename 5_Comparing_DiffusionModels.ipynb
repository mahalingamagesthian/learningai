{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvRALcsMuvuMeliHBWFm6c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahalingamagesthian/learningai/blob/main/5_Comparing_DiffusionModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model is inspired from https://www.marktechpost.com/2025/05/05/a-coding-guide-to-compare-three-stability-ai-diffusion-models-v1-5-v2-base-sd3-medium-diffusion-capabilities-side-by-side-in-google-colab-using-gradio/ Whilst this run from Google Colab, there are some restrictions on the GPU Usage. Hence the comparison is limited to two models."
      ],
      "metadata": {
        "id": "LlXE5PPDgTpq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "04SZJ_4XNUYE"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# This will pop up for your token from huggingface\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gradio\n",
        "!pip install gradio"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DsK8y2GGCA9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusion3Pipeline\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "id": "nzXYQu1UEUcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NcCpeCDSFatU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe1 = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16,\n",
        "    safety_checker=None\n",
        ").to(device)\n",
        "pipe1.enable_attention_slicing()"
      ],
      "metadata": {
        "id": "53M059LNFYue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe2 = StableDiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-2-base\",\n",
        "    torch_dtype=torch.float16,\n",
        "    safety_checker=None\n",
        ").to(device)\n",
        "pipe2.enable_attention_slicing()"
      ],
      "metadata": {
        "id": "syJa6KwmFkcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to generate the image based on the prompt\n",
        "def generate(prompt, steps, scale):\n",
        "    img1 = pipe1(prompt, num_inference_steps=steps, guidance_scale=scale).images[0]\n",
        "    img2 = pipe2(prompt, num_inference_steps=steps, guidance_scale=scale).images[0]\n",
        "    return img1, img2"
      ],
      "metadata": {
        "id": "HD27Aqb1NZnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose(selection):\n",
        "    return f\"âœ… You selected: **{selection}**\"\n",
        "\n",
        "#This will be a Online Form, Clicking on \"Generate Images\" will create the images\n",
        "#in the boxes.\n",
        "# Prompt can be changed to recreate more pictures.\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## AI Social-Post Generator with 2 Models\")\n",
        "    with gr.Row():\n",
        "        prompt = gr.Textbox(label=\"Prompt\", placeholder=\"a fantasy landscape with castles and dragons\")\n",
        "        steps  = gr.Slider( 1, 100, value=50, step=1,     label=\"Inference Steps\")\n",
        "        scale  = gr.Slider( 1.0, 20.0, value=7.5, step=0.1, label=\"Guidance Scale\")\n",
        "    btn = gr.Button(\"Generate Images\")\n",
        "    with gr.Row():\n",
        "        out1 = gr.Image(label=\"Model 1: SD v1.5\")\n",
        "        out2 = gr.Image(label=\"Model 2: SD v2-base\")\n",
        "    sel = gr.Radio(\n",
        "        [\"Model 1: SD v1.5\",\"Model 2: SD v2-base\"],\n",
        "        label=\"Select your favorite\"\n",
        "    )\n",
        "    txt = gr.Markdown()\n",
        "\n",
        "\n",
        "    btn.click(fn=generate, inputs=[prompt, steps, scale], outputs=[out1, out2])\n",
        "    sel.change(fn=choose, inputs=sel, outputs=txt)\n",
        "\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "EyCVRavUNlOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TroubleShooting**\n",
        "*Enable Access to hugging face model*\n",
        "Cannot access gated repo for url https://huggingface.co/stabilityai/stable-diffusion-3-medium-diffusers/resolve/main/model_index.json.\n",
        "Access to model stabilityai/stable-diffusion-3-medium-diffusers is restricted and you are not in the authorized list. Visit https://huggingface.co/stabilityai/stable-diffusion-3-medium-diffusers to ask for access.\n",
        "\n",
        "*CUDA Out of Memory* OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 130.12 MiB is free. Process 2412 has 14.61 GiB memory in use. Of the allocated memory 14.39 GiB is allocated by PyTorch, and 125.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
      ],
      "metadata": {
        "id": "XfSHrVKsIvSk"
      }
    }
  ]
}