{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c41c9b8-ae86-4cb1-942a-3e76bc7620db",
   "metadata": {},
   "source": [
    "Let's break down foundation models and then set up a basic Hugging Face text-to-text generation model that you can run on a JupyterHub platform.\n",
    "\n",
    "# Understanding Foundation Models #\n",
    "Foundation models are large artificial intelligence (AI) models trained on vast amounts of data. These models are designed to be versatile and can be adapted (fine-tuned) for a wide range of downstream tasks. Instead of training a specific model for each task from scratch, foundation models learn general representations from broad data and can then apply this knowledge to new, specific problems.\n",
    "\n",
    "# Key Characteristics of Foundation Models: #\n",
    "\n",
    "Trained on Broad Data: They learn from massive datasets, often using self-supervised learning where the model learns from unlabeled data.\n",
    "`General-Purpose`: They are not designed for a single task but can perform many different tasks, such as language understanding, text generation, image recognition, and more.\n",
    "Adaptable (via Fine-tuning): They can be customized for specific applications with smaller task-specific datasets. This is more efficient than training a new large model from scratch.\n",
    "Emergent Capabilities: Sometimes, foundation models exhibit abilities that were not explicitly programmed or anticipated during training, arising from their scale and the nature of the data they've learned from.\n",
    "\n",
    "## Examples of Foundation Models: ##\n",
    "\n",
    "Large Language Models (LLMs): Models like GPT-3, GPT-4, Llama, and others, which excel at understanding and generating human language.\n",
    "Vision Models: Models like CLIP that understand the relationship between images and text.\n",
    "Multimodal Models: Models that can process and generate information across different modalities, such as text and images (e.g., DALL-E, some newer models).\n",
    "Applications of Foundation Models:\n",
    "\n",
    "Foundation models are being used in a wide array of applications, including:\n",
    "\n",
    "Natural Language Processing (NLP): Text generation, translation, summarization, question answering, chatbots, content creation.\n",
    "Computer Vision: Image generation, image captioning, object detection, image classification.\n",
    "Code Generation: Assisting developers with writing and debugging code.\n",
    "Drug Discovery: Predicting molecular properties and designing new drug candidates.\n",
    "Robotics: Enabling robots to understand instructions and interact with their environment.\n",
    "\n",
    "Basic Setup for a Hugging Face Text-to-Text Generation Model in JupyterHub\n",
    "Hugging Face's transformers library provides easy access to thousands of pre-trained foundation models, including many powerful text-to-text generation models. Here's a basic setup to run one in a JupyterHub environment using Python:\n",
    "\n",
    "### Prerequisites: ###\n",
    "\n",
    "JupyterHub Access: You need to have access to a JupyterHub instance.\n",
    "Python Environment: Ensure your Jupyter environment has Python installed.\n",
    "transformers Library: Install the Hugging Face transformers library. You can do this in a Jupyter notebook cell by running:\n",
    "Python\n",
    "\n",
    "`!pip install transformers`\n",
    "\n",
    "torch (Recommended): Many transformer models are built using PyTorch. Install it if you haven't:\n",
    "Python\n",
    "\n",
    "`!pip install torch`\n",
    "\n",
    "verify the certificate validity checks\n",
    "\n",
    "import requests\n",
    "import certifi\n",
    "print(certifi.where())\n",
    "try:\n",
    "    #response = requests.get(\"https://huggingface.co\", verify='/tmp/huggingface.cert')\n",
    "    #response = requests.get(\"https://quay.io\", verify='/tmp/quay.io.cert')\n",
    "    response = requests.get(\"https://huggingface.co\", verify=certifi.where())\n",
    "    response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "    print(\"Connection successful!\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "download the certificate with certificate chain from https://huggingface.co, upload it to the .cem file as below\n",
    "huggingface.co.pem , Tesco_Secure_Proxy.pem & Tesco Root CA.pem are to be added to the CA Truststore `lib/python3.11/site-packages/certifi/cacert.pem`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3fb6859-db63-4468-bafe-f93463bdc9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# Specify the text-to-text generation model you want to use\n",
    "# A good starting point for general text-to-text tasks is 't5-small'\n",
    "model_name = \"t5-small\"\n",
    "# Create a text-to-text generation pipeline\n",
    "generator = pipeline(\"text2text-generation\", model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "999a8ddd-a9a4-4179-8d02-39a1ec303cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quelle est la capitale du Karnataka?\n"
     ]
    }
   ],
   "source": [
    "# Your input text\n",
    "input_text = \"translate English to French: What is the capital of Karnataka? \"\n",
    "\n",
    "# Generate the output\n",
    "output = generator(input_text, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Print the generated output\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5cf8bd2-2821-44df-9e36-c054ded684e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mission is the 128th space shuttle mission and the 33rd flight of Discovery . the crew will deliver the multi-purpose logistics module (MPLM) Leonardo as its primary payload .\n"
     ]
    }
   ],
   "source": [
    "# You can try different input texts and see the model's output\n",
    "input_text_2 = \"summarize: The Orbiter Discovery, STS-128 mission is the 128th Space Shuttle mission and the 33rd flight of Discovery. The seven-member crew will deliver the Multi-Purpose Logistics Module (MPLM) Leonardo as its primary payload, along with several transfer assemblies (ITAS), the Lightweight Multi-purpose Experiment Support Structure Carrier (LMC) and two experiments.\"\n",
    "output_2 = generator(input_text_2, max_length=100, num_return_sequences=1)\n",
    "print(output_2[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1b86d01-d004-485a-8d90-7db109f4daff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berlin\n"
     ]
    }
   ],
   "source": [
    "# You can also try question answering\n",
    "input_text_3 = \"question: What is the capital of Germany? context: Berlin is the capital and largest city of Germany by both area and population.\"\n",
    "output_3 = generator(input_text_3, max_length=30, num_return_sequences=1)\n",
    "print(output_3[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b08ca6-76c3-4550-8eae-5c27e001ac2b",
   "metadata": {},
   "source": [
    "# Explanation of the Code: #\n",
    "\n",
    "`from transformers import pipeline`: This line imports the pipeline function from the transformers library. Pipelines are a high-level abstraction that simplifies using pre-trained models for various tasks.\n",
    "\n",
    "`model_name = \"t5-small\"`: This line specifies the name of the pre-trained text-to-text generation model you want to use. \"t5-small\" is a relatively small and efficient version of the T5 (Text-to-Text Transfer Transformer) model, which is known for its versatility across different text-based tasks. You can explore other models on the Hugging Face Model Hub (https://huggingface.co/models?pipeline_tag=text2text-generation). Some other popular choices include facebook/bart-large-cnn (for summarization) or larger T5 models for potentially better performance but higher computational cost. Â  \n",
    "\n",
    "`generator = pipeline(\"text2text-generation\", model=model_name)`: This is the core of the setup. It creates a text-to-text generation pipeline using the specified pre-trained model. When you run this for the first time, the model weights will be downloaded from the Hugging Face Model Hub and cached locally.\n",
    "\n",
    "`input_text = ...`: Here, you define the text you want the model to process. The format of the input often depends on the specific task and the way the model was trained. For T5, it's common to prepend a task-specific prefix (e.g., \"translate English to French:\", \"summarize:\", \"question: ... context: ...\").\n",
    "\n",
    "`output = generator(input_text, max_length=50, num_return_sequences=1)`: This line feeds the input_text to the generator to produce the output.\n",
    "\n",
    "`max_length`: This parameter controls the maximum length of the generated output sequence. Adjust it based on the expected length of the output for your task.\n",
    "num_return_sequences: This parameter specifies how many different output sequences the model should generate. Here, we are asking for just one.\n",
    "print(output[0]['generated_text']): The output of the generator is a list of dictionaries. Each dictionary contains the generated text under the key 'generated_text'. We print the first (and in this case, only) generated text.\n",
    "\n",
    "`Trying Different Inputs`: The subsequent examples demonstrate how you can use the same pipeline for different text-based tasks by changing the input_text and the task-specific prefix.\n",
    "\n",
    "# Running in JupyterHub: #\n",
    "\n",
    "Open a new Jupyter notebook in your JupyterHub environment.\n",
    "Copy and paste the code into a cell.\n",
    "Run the cell. The first time you run it, it might take a few moments to download the model. Subsequent runs will be faster as the model is cached.\n",
    "Experiment with different model_name values and different input_text prompts to explore the capabilities of various text-to-text generation models. You can also adjust the max_length and num_return_sequences parameters.\n",
    "This basic setup provides a starting point for exploring text-to-text generation with Hugging Face in your JupyterHub environment. As you become more familiar, you can delve into more advanced techniques like fine-tuning models on your own datasets or customizing the generation parameters for more control over the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d945eb1-2e40-4d68-9d7f-fe6ffb6fb325",
   "metadata": {},
   "source": [
    "# Troubleshooting #\n",
    "\n",
    "__sympy.printing__\n",
    "\n",
    "RuntimeError: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\n",
    "module 'sympy' has no attribute 'printing'\n",
    "\n",
    "_Solution_\n",
    "Go to the python file torch/utils/_sympy/functions.py add the import code in the top line: \n",
    "from sympy import printing\n",
    "\n",
    "__Xet Storage__\n",
    "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
    "\n",
    "`!pip install hf_xet --trusted-host pypi.org --trusted-host files.pythonhosted.org`\n",
    "\n",
    "\n",
    "Run the below code to confirm that the progress bar is working fine:\n",
    "\n",
    "ImportError: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
    "\n",
    "!pip install ipywidgets --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "!pip install jupyterlab-widgets --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "\n",
    "Restart the jupyterlab. Run the below code to confirm that the progress bar is working fine:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c732d11d-02ae-44f8-b85b-fc7ad0644d96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
