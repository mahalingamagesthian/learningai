{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahalingamagesthian/learningai/blob/main/7_Agentic_LangGraph_YT_LangGraph_ReAct_Pattern.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference:\n",
        "\n",
        "https://www.youtube.com/watch?v=pEMhPBQMNjg&list=PL8motc6AQftlE8U8IQrftOUyzI_00PObk\n",
        "\n",
        "https://langchain-ai.github.io/langgraph/concepts/why-langgraph/\n",
        "\n",
        "This pip install command sets up a complete environment for building a `LangGraph` agent that leverages OpenAI's models and can perform web searches using `DuckDuckGo`. It pulls in all the foundational `LangChain` components, the core `LangGraph` orchestration engine, the specific connector for `OpenAI`, and a practical tool for your agent to use.\n",
        "\n",
        "This combination allows you to build agents that can:\n",
        "\n",
        " 1. Understand complex prompts.\n",
        " 2. Decide which actions to take.\n",
        " 3. Use a search engine to find information.\n",
        " 4. Reason over the results.\n",
        " 5. And potentially loop or retry based on a dynamic graph of operations.\n",
        "\n",
        "\n",
        "# Install Relevant Libraries\n",
        "\n",
        "\n",
        " Let's break down each part of that pip install command, explaining its role in the LangGraph/LangChain ecosystem,\n",
        "\n",
        "## langchain-core\n",
        "\n",
        "**Role:** This is the foundational library of the LangChain ecosystem. It contains the core abstractions and base classes that underpin almost everything in LangChain and LangGraph.\n",
        "\n",
        "**What it provides:**\n",
        " * `Base interfaces`: Defines fundamental concepts like `BaseLLM` (for language models), `BasePromptTemplate` (for prompts), `BaseOutputParser` (for structured output), `BaseTool` (for tools that agents use), and `BaseMemory`(for conversational memory).\n",
        " * `Runnables`: The LangChain Expression Language (LCEL) is built on top of langchain-core's runnables, which allow you to compose chains and pipelines in a flexible and efficient way.\n",
        " * `Callbacks` : The mechanism for monitoring and observing your LLM application's execution.\n",
        "\n",
        "**`Why you need it`**:\n",
        "Without langchain-core, you wouldn't have the fundamental building blocks to define and interact with any LLM component in a standardized way. It's the absolute base layer.\n",
        "\n",
        "\n",
        "## langchain-community\n",
        "\n",
        "**Role**: This library provides integrations with a vast array of third-party providers and utilities. It houses the concrete implementations of the abstract interfaces defined in `langchain-core`.\n",
        "\n",
        "**What it provides**:\n",
        "* `Specific LLM integrations`: While `langchain-openai` (see next) handles OpenAI, `langchain-community` provides integrations for many other LLMs like Hugging Face Hub (as discussed before), Anthropic, Cohere, Google Gemini, etc.\n",
        "* `Document loaders`: Classes to load data from various sources (PDFs, websites, databases, CSVs).\n",
        "* `Vector stores`: Integrations with vector databases (e.g., Chroma, FAISS, Pinecone) for RAG (Retrieval Augmented Generation).\n",
        "* `Tools`: Implementations for common tools an agent might use (e.g., `DuckDuckGoSearchRun` which is pulled in by `duckduckgo-search` but often conceptually part of `langchain-community`'s tool ecosystem).\n",
        "\n",
        "**Why you need it**: It provides the specific \"pluggable parts\" that allow your `LangChain/LangGraph` application to interact with external services, load data, and use various models beyond just OpenAI.\n",
        "\n",
        "\n",
        "## langgraph\n",
        "\n",
        "**Role:** This is the specialized library for building stateful, multi-actor, and highly controllable agentic applications using a graph-based approach. As we discussed earlier, it extends LangChain's capabilities for complex workflows.\n",
        "\n",
        "**What it provides:**\n",
        "* `Graph Abstraction`: Allows you to define `nodes` (steps in your agent's thought process) and `edges` (transitions between steps).\n",
        "* `State Management` : Built-in mechanisms to manage and update a shared application state as the agent moves through the graph.\n",
        "* `Cycles & Conditionals`: Enables sophisticated control flow, including loops (for retries or iterative refinement) and conditional branching.\n",
        "* `Human-in-the-loop`: Facilitates adding human intervention points.\n",
        "\n",
        "**`Why you need it`**:  LangGraph is precisely what you need to orchestrate more complex, autonomous behaviors that go beyond simple sequential chains.\n",
        "\n",
        "## langchain-openai\n",
        "\n",
        "**Role**: This is a specific integration package that provides the classes and methods for interacting with OpenAI's Large Language Models (LLMs) and Chat Models via their API.\n",
        "\n",
        "**What it provides:**\n",
        "* Implementations for `OpenAI` (for text completion models like text-davinci-003) and `ChatOpenAI` (for chat models like gpt-3.5-turbo, gpt-4).\n",
        "* Tools that might specifically leverage OpenAI's APIs.\n",
        "\n",
        "**`Why you need it`**: If your Agentic AI model (or any part of it) uses OpenAI models for its reasoning, generation, or tool calls, this package is essential for connecting to their API.\n",
        "\n",
        "## duckduckgo-search\n",
        "\n",
        "**Role**: This is a Python library that provides a programmatic way to perform web searches using `DuckDuckGo's` search engine.\n",
        "\n",
        "**What it provides:**\n",
        "* Functions to send queries to DuckDuckGo and parse the search results.\n",
        "\n",
        "**Why you need it**: In the context of an agent, a search tool is incredibly common. Agents often need to look up real-world information that isn't contained in their training data or memory. This package provides the \"tool\" that your `LangGraph` agent can use to perform a web search."
      ],
      "metadata": {
        "id": "KTYitDHjengD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain-core langchain-community langgraph langchain-openai duckduckgo-search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVYp7LLzhDDM",
        "outputId": "9eeac48f-f69d-4212-fed4-0646e1f4590f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangGraph ReAct Function Calling Pattern\n",
        "\n",
        "- Search\n",
        "\n",
        "- Math\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_PK5Dpx4j02e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Traditional ReAct\n",
        "\n",
        "\n",
        "react_prompt = \"\"\"Assistant is a large language model trained by Microsoft.\n",
        "\n",
        "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
        "\n",
        "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
        "\n",
        "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
        "\n",
        "TOOLS:\n",
        "------\n",
        "\n",
        "Assistant has access to the following tools:\n",
        "\n",
        "wikipedia_search - searches the wikipedia database for the answer\\n\n",
        "web_search - searches the web for the answer\\n\n",
        "calculator - calculates the answer to the question\\n\n",
        "weather_api - gets the weather for the location\\n\n",
        "\n",
        "\n",
        "To use a tool, please use the following format:\n",
        "\n",
        "```\n",
        "Thought: Do I need to use a tool? Yes\n",
        "Action: the action to take, should be one of [wikipedia_search, web_search, calculator, weather_api]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "```\n",
        "\n",
        "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
        "\n",
        "```\n",
        "Thought: Do I need to use a tool? No\n",
        "Final Answer: [your response here]\n",
        "```\n",
        "\n",
        "Begin!\n",
        "\n",
        "\n",
        "New input: Whow was King Arthur?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "o0vQNgwBKU0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing OPEN API Key\n",
        "\n",
        "This small code snippet is crucial for securely managing your API keys, especially when working in environments like Google Colab.\n",
        "\n",
        "1. `import os`\n",
        "\n",
        "* `os module`: This is a built-in Python module that provides a way of using operating system-dependent functionality. It allows you to interact with the underlying operating system.\n",
        "\n",
        "* `Key Functionality`: One of its primary uses is to interact with environment variables. Environment variables are dynamic named values that can affect the way running processes behave. They are part of the operating system's environment.\n",
        "\n",
        "* `Why it's needed here`: We'll use `os.environ` to set an environment variable, which is a standard and secure way to pass sensitive information (like API keys) to applications without hardcoding them directly into the code.\n",
        "\n",
        "2. `from google.colab import userdata`\n",
        "\n",
        "* `google.colab`: This module is specific to the Google Colaboratory environment. Colab is a free cloud-based Jupyter notebook environment that runs on Google's infrastructure.\n",
        "* `userdata`: This is a sub-module within `google.colab` that provides a secure way to store and retrieve sensitive user data, such as API keys, directly within your Colab environment.\n",
        "* `Significance`: Instead of hardcoding your OPENAI_API_KEY directly into your notebook (which is a major security risk, especially if you share the notebook), Colab's `userdata` module allows you to store it securely. When you use `userdata.get('OPENAI_API_KEY')`, Colab retrieves the value you previously saved in its **secrets manager**.\n",
        "\n",
        "3. os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "* `userdata.get('OPENAI_API_KEY')`: This part fetches the value of the secret named OPENAI_API_KEY that you have securely stored in your Google Colab **\"Secrets\"** tab.\n",
        "* `How to store it in Colab`:\n",
        "  * In your Google Colab notebook, look for a key icon on the left sidebar (usually towards the bottom, often labeled \"Secrets\" or \"Keys\").\n",
        "  * Click on it.\n",
        "  * Click \"Add new secret\".\n",
        "  * For \"Name\", type OPENAI_API_KEY (case-sensitive).\n",
        "  * For \"Value\", paste your actual OpenAI API key (the one you obtained from the OpenAI platform).\n",
        "  * Make sure the \"Notebook access\" toggle is ON for the current notebook.\n",
        "\n",
        "`os.environ['OPENAI_API_KEY'] = ...`: This line takes the API key retrieved from `userdata.get('OPENAI_API_KEY')` and sets it as an environment variable within the Python process where your notebook is running.\n",
        "`Why an environment variable`? Many libraries, including `langchain-openai`, are designed to automatically look for API keys in environment variables (e.g., OPENAI_API_KEY). By setting it this way, you don't have to explicitly pass the key to every function call; the library will find it automatically. This is a common and secure pattern for managing credentials in applications.\n",
        "\n",
        "In essence, this block of code is performing a critical security and configuration step:\n",
        "\n",
        "It retrieves your sensitive OpenAI API key from a secure, Colab-specific storage mechanism (`userdata`) and then makes it available to your Python application (and libraries like LangChain) by setting it as an environment variable. This prevents you from exposing your API key in your public code or committing it to version control systems like `Git`, which is a fundamental security best practice in software development."
      ],
      "metadata": {
        "id": "hvYc_3EhrhzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n"
      ],
      "metadata": {
        "id": "myACJtH5hDrT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connecting to OpenAI\n",
        "Establishes a connection (or at least prepares to connect) to OpenAI's API. Specifies that you intend to use the gpt-4o model.\n",
        "Creates an llm object which is now a callable interface. You can pass a `HumanMessage` (or a list of messages) to this llm object, and it will send that request to OpenAI's `gpt-4o model`, then return the model's response.\n",
        "\n",
        "# `from langchain_openai import ChatOpenAI`:\n",
        "\n",
        "* `langchain_openai`: This is a specific Python package within the larger LangChain ecosystem.`langchain_openai` is its dedicated integration module for interacting with OpenAI's APIs.\n",
        "ChatOpenAI: This is a specific class provided by the langchain_openai package.\n",
        " * `Purpose`: It's designed to interface with OpenAI's \"chat models\". These are models optimized for multi-turn conversations and often perform better for dialogue-based tasks, instruction following, and complex reasoning (e.g., gpt-3.5-turbo, gpt-4, gpt-4o).\n",
        " * `Contrast with OpenAI class`: There's also an OpenAI class in langchain_openai which is typically used for older \"completion\" models (like text-davinci-003) that take a single prompt and generate a completion. For modern LLM applications, ChatOpenAI is generally preferred as it uses the more capable chat completion APIs.\n",
        "\n",
        "* `llm = ChatOpenAI(model=\"gpt-4o\"):`\n",
        "\n",
        "  * `llm variable`: This is a common naming convention in LangChain to refer to an \"Large Language Model\" instance. It's an object that represents your connection to a specific LLM.\n",
        "  * `ChatOpenAI(...)`: This is the constructor call to create an instance of the ChatOpenAI class.\n",
        "  * `model=\"gpt-4o\"`: This is a crucial argument passed to the constructor. It specifies which particular OpenAI chat model you want to use for this instance.\n",
        "\n",
        "`gpt-4o (GPT-4 Omni)` is OpenAI's latest flagship model at the time of writing, known for its advanced capabilities in understanding and generating text, vision, and soon, audio.\n",
        "If you wanted to use a different model, like gpt-4-turbo or gpt-3.5-turbo, you would change this string.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ckRUOJLgvCNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ],
      "metadata": {
        "id": "Iv9zB6BrhbRY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tools\n",
        "In the next step because it introduces the concept of tools for your AI agent. In LangChain and LangGraph, agents often don't just rely on their language model's internal knowledge; they can use external functions or APIs to perform specific tasks.\n",
        "\n",
        "These are just three standard Python function definitions. They implement basic arithmetic operations:\n",
        "\n",
        "1. multiply(a: int, b: int) -> int:\n",
        "\n",
        "* Takes two integer inputs (a and b).\n",
        "* Returns their product as an integer.\n",
        "* The \"\"\"Docstring\"\"\" explains what the function does, its arguments, and their types. This docstring is crucially important for LangChain/LangGraph.\n",
        "2. add(a: int, b: int) -> int:\n",
        "\n",
        "* Takes two integer inputs (a and b).\n",
        "* Returns their sum as an integer.\n",
        "3. divide(a: int, b: int) -> float:\n",
        "\n",
        "* Takes two integer inputs (a and b).\n",
        "* Returns their quotient as a floating-point number. Note the  float return type, which is important for division.\n",
        "\n",
        "**Significance in the context of LangChain/LangGraph and Agentic AI:**\n",
        "\n",
        "The comment `# This will be a tool` is the most important clue here. These simple Python functions are being prepared to be exposed as `\"tools\"` to your Large Language Model (LLM) agent.\n",
        "\n",
        "**How LLMs use tools:**\n",
        "\n",
        "1. `Tool Definition`: In LangChain/LangGraph, you'll convert these Python functions into a `Tool` object (usually using create_tool or tool decorator).\n",
        "2. `Tool Description`: The `docstrings (\"\"\"Multiply a and b...\"\"\")` of these functions are absolutely critical. `LangChain` will extract these docstrings and present them to the LLM as a \"description\" of what the tool does, its arguments, and their types.\n",
        "3. `LLM Reasoning`: When your agent (powered by `gpt-4o` in your case) receives a user query (e.g., \"What is 50 divided by 5, plus 10?\"), it will:\n",
        "* `Analyze the query`: It understands that it needs to perform mathematical operations.\n",
        "* `Consult its available tools`: It looks at the descriptions of the multiply, add, and divide tools you've given it.\n",
        "* `Decide which tool to use`: Based on its training and the tool descriptions, it \"decides\" that divide is appropriate for \"50 divided by 5\".\n",
        "* `Format the tool call`: It generates a structured output (often in a specific JSON format) telling the framework to call the divide tool with a=50 and b=5.\n",
        "\n",
        "4. `Tool Execution`: The `LangChain/LangGraph` framework intercepts this structured output, calls your actual Python divide function with the specified arguments.\n",
        "5. `Result Observation`: The framework then takes the result (10.0 in this case) and feeds it back to the LLM as \"observation.\"\n",
        "6. `Further Reasoning`: The LLM then sees the observation (\"The division result is 10.0\") and can continue its reasoning (e.g., \"Now I need to add 10 to 10.0, so I'll call the add tool\").\n",
        "\n",
        "# Why expose functions as tools?\n",
        "\n",
        "* `Overcoming LLM Limitations`: While LLMs are great at text generation, they are not reliable calculators. They can \"hallucinate\" math results. By giving them explicit calculator tools, you leverage their strong language understanding for reasoning and tool selection, and the tool's deterministic accuracy for computation.\n",
        "* `Accessing External Capabilities:` Tools are how LLMs can \"interact with the world\" beyond their training data. This includes:\n",
        "  * Searching the web (`duckduckgo-search`).\n",
        "  * Accessing databases.\n",
        "  * Calling external APIs (e.g., weather APIs, stock APIs).\n",
        "  * Running code interpreters.\n",
        "* `Extensibility`: You can extend your agent's capabilities simply by writing a new Python function and exposing it as a tool.\n",
        "\n",
        "These three simple Python functions are the foundation of external capabilities for your AI agent. They represent the \"skills\" or \"actions\" that your intelligent agent can perform by making precise, controlled calls to code, rather than trying to perform those operations itself purely through text generation."
      ],
      "metadata": {
        "id": "Xjw2pzHJmgcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "# This will be a tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Adds a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "def divide(a: int, b: int) -> float:\n",
        "    \"\"\"Divide a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a / b"
      ],
      "metadata": {
        "id": "1AJoWmexhbUQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet introduces a very practical and common type of tool an AI agent might need: **a search engine**.\n",
        "\n",
        "1.`# search tools`\n",
        "\n",
        "This is a comment, indicating that the following code is related to setting up search functionalities, which are indeed a type of tool for an agent.\n",
        "\n",
        "2.`from langchain_community.tools import DuckDuckGoSearchRun`\n",
        "\n",
        " * `langchain_community.tools`: `langchain_community` is where many third-party integrations and concrete implementations of tools reside. `tools` is a sub-module specifically for various types of tools that can be given to LLMs.\n",
        " * `DuckDuckGoSearchRun`: This is a specific class provided by `langchain_community` that implements a web search tool using the `DuckDuckGo` search engine.\n",
        "  * `Purpose`: It encapsulates the logic required to send a query to `DuckDuckGo`, receive the search results, and format them in a way that an LLM can easily consume as an `\"observation.\"`\n",
        "\n",
        "3. `search = DuckDuckGoSearchRun()`\n",
        "\n",
        "This line creates an instance of the `DuckDuckGoSearchRun` class.\n",
        "The search variable now holds an object that represents the `DuckDuckGo` search tool. This object is ready to be used.\n",
        "`search.invoke(\"How old is Brad Pitt?\")`\n",
        "\n",
        "4. `search.invoke(...)`: This is the core action. In `LangChain (and especially in LangGraph)`, `invoke` is the standard method used to execute a runnable or a tool.\n",
        "5. `\"How old is Brad Pitt?\"`: This is the input to the invoke method, which in this case is the search query you want to send to `DuckDuckGo`.\n",
        "\n",
        "## What this code does in simple terms:\n",
        "\n",
        "This code effectively acts as a direct call to the `DuckDuckGo` search engine through a convenient `LangChain` wrapper. It performs the following sequence of operations:\n",
        "\n",
        " * Initializes the `DuckDuckGo` search tool.\n",
        " * Sends the query `\"How old is Brad Pitt?\"` to the `DuckDuckGo` search engine.\n",
        " * Waits for `DuckDuckGo` to return its search results.\n",
        " * Processes these results: `DuckDuckGoSearchRun` will typically extract relevant snippets or summaries from the search results.\n",
        " * Returns the processed search results: The output of search.invoke(...) will be a string containing information found by `DuckDuckGo` for the given query.\n",
        "\n",
        "## Significance for an Agentic AI model:\n",
        "\n",
        "This search tool is a prime example of how an LLM agent **gains access to real-time, external information**.\n",
        "\n",
        " * `Bridging Knowledge Gaps`: An LLM's internal knowledge is limited to its training data, which can become outdated. By providing a search tool, the agent can look up current events, specific facts, or detailed information it doesn't already know.\n",
        " * `Problem-Solving`: When faced with a question it can't answer directly from its internal knowledge, an intelligent agent can decide to use the search tool, process the search results, and then formulate a comprehensive answer or decide on its next action.\n",
        " * `Tool Calling`: In an actual agent implementation (especially with `LangGraph`), the LLM wouldn't directly call `search.invoke()`. Instead, the LLM would `\"decide\"` (based on its prompt and the tool's description) that it needs to perform a search, generate a structured `\"tool call\" (e.g., tool_name=\"duckduckgo_search\", input=\"how old is brad pitt\")`, and the LangChain/LangGraph framework would then intercept that call and execute the `search.invoke()` for it. The result would then be passed back to the LLM.\n",
        "\n",
        " This snippet demonstrates a *fundamental building block for giving your AI agent the ability to act on and retrieve information from the outside world*."
      ],
      "metadata": {
        "id": "06j51-UuAsIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# search tools\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "\n",
        "search = DuckDuckGoSearchRun()\n",
        "\n",
        "search.invoke(\"How old is Brad Pitt?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "1VfRa-pdmm1u",
        "outputId": "127db69c-866e-4126-b867-ba9a03c2c5b0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The 60-year-old Pitt portrays Nick, ... Brad Pitt was born William Bradley Pitt on December 18, 1963, in Shawnee, Oklahoma. The eldest of three children in a devoutly Southern Baptist family, Brad ... Brad Pitt has commented publicly for the first time on the contentious and lengthy divorce proceedings he went through after separating from ex-wife Angelina ... The 61-year-old star responded ... Brad Pitt (born December 18, 1963, Shawnee, Oklahoma, U.S.) is an American actor known for his portrayal of unconventional characters. After gaining attention for his role in Thelma & Louise (1991), Pitt channeled his magnetism as a leading man in such movies as A River Runs Through It (1992) and Legends of the Fall (1994) but avoided being ... Zahara Marley Jolie-Pitt was born on Jan. 8, 2005, in Hawassa, Ethiopia, and was adopted by Jolie when she was seven-months-old. Jolie initially visited Zahara's orphanage with Maddox after he had ... Brad Pitt's net worth: Mostly from his acting career, producing work, and other commercial projects, Brad Pitt's net worth is expected to be $300 million by 2025. Brad Pitt's age is: Born December 18, 1963, Brad Pitt is 61 years old. Brad Pitt's height is: Brad Pitt rises to five feet eleven inches (180 cm). Brad Pitt has what number of ...\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " This is a crucial step in preparing your Language Model (LLM) to become an `\"agent\"` that can use external functionalities. It's where you formally tell your `gpt-4o` model instance about the tools it has at its disposal.\n",
        "\n",
        " 1. tools = [add, multiply, divide, search]\n",
        "\n",
        " * `tools variable`: This line simply creates a standard Python list.\n",
        " * `[add, multiply, divide, search]`: This list contains the actual Python function objects `(add, multiply, divide )` and the `DuckDuckGoSearchRun` object (search) you initialized.\n",
        " * It's important to note that these are the functions themselves and the tool instance, not just their names as strings.\n",
        "\n",
        "**What this line does**: It gathers all the individual functions and tool instances that you want your LLM to be able to use into a single collection.\n",
        "\n",
        "2. llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        " * `llm`: This refers to the `ChatOpenAI` instance you created earlier `(llm = ChatOpenAI(model=\"gpt-4o\"))`. This `llm` object is capable of communicating with the `gpt-4o` model via OpenAI's API.\n",
        "\n",
        " * `.bind_tools(tools)`: This is the core operation. It's a method provided by `LangChain's` LLM objects (specifically, `ChatOpenAI` in this case).\n",
        "\n",
        " * **Purpose**: This method takes a list of tools  and \"binds\" them to the language model.\n",
        " * **How it works (internally)**: When you call bind_tools, LangChain doesn't actually give the code of your add, multiply, divide, or search functions directly to gpt-4o. Instead, it does something very clever:\n",
        "    1. `Extracts Tool Schemas`: It inspects each function/tool in the tools list. For your Python functions (add, multiply, divide), `LangChain` uses their docstrings (the `\"\"\"Add a and b.\"\"\" part) and type hints (a: int, b: int -> int)` to automatically generate a structured description of what the tool does, its name, and its expected input parameters (including their types).\n",
        "    2. For the `search` tool (which is already a `LangChain` Tool object), it already has this structured description built-in.\n",
        "    3. `Passes as System/Tool Prompt`: When you later make a call to `llm_with_tools`, `LangChain` will automatically inject these generated tool descriptions into the system message or a specific tool prompt that it sends to the gpt-4o model. This tells the LLM: \"Hey, when you're thinking about how to respond, here are some external functions/tools you can call, and here's how to call them.\"\n",
        "    4. `Enables Function Calling`: Modern LLMs like gpt-4o are fine-tuned for \"function calling\" or \"tool use.\" This means they can respond not just with natural language but also with a structured output (e.g., JSON) that indicates a tool call.\n",
        " * `llm_with_tools`: This new variable holds a new LLM object that is essentially the original llm but now \"aware\" of the tools. When you send a query to `llm_with_tools`, it knows that it can use these tools, and if it decides to, it will output a structured tool call instead of just a natural language response.\n",
        "\n",
        " **In essence, this line of code is performing the critical setup that allows your gpt-4o model to transition from being just a text generator to an intelligent agent capable of interacting with external functionalities. It's the process of giving your LLM \"skills\" by describing the tools it has available.**\n",
        "\n",
        "Now, when you use `llm_with_tools` in your agent's workflow, the LLM will be prompted not only to answer the user's question but also to consider if calling `add, multiply, divide, or search` would help it arrive at a better answer or perform a necessary action."
      ],
      "metadata": {
        "id": "f59cvHUmFYm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [add, multiply, divide, search]\n",
        "\n",
        "llm_with_tools = llm.bind_tools(tools)"
      ],
      "metadata": {
        "id": "7rlu8MF-ho6e"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This LangGraph Agent code defines the initial state and the persona of your AI assistant.\n",
        "\n",
        "# Understanding the Imports\n",
        "1. `from langgraph.graph import MessagesState:`\n",
        "\n",
        "  * `langgraph.graph`: This is the core module within the `LangGraph` library where the fundamental building blocks for creating graphs are defined.\n",
        "  * `MessagesState`: This is a specific type of state that `LangGraph` provides out-of-the-box. When you build a graph, you need to define what kind of information (or \"state\") your agent will maintain and pass between its different steps (nodes). `MessagesState` is designed for conversational agents. It automatically manages a list of `BaseMessage` objects (like `HumanMessage`, `AIMessage`, `SystemMessage`), which is the standard way `LangChain` represents conversational turns.\n",
        "  * `Significance`: By using `MessagesState`, your agent will naturally keep track of the conversation history, allowing it to respond contextually and remember previous interactions. This is essential for any assistant that needs to hold a conversation.\n",
        "\n",
        "2. `from langchain_core.messages import HumanMessage, SystemMessage`:\n",
        "\n",
        " * `langchain_core.messages`: This module from `langchain-core` defines the different types of messages that can be part of a conversation with an LLM.\n",
        " * `HumanMessage`: Represents a message from the user (or human).\n",
        " * `SystemMessage`: Represents a message that sets the context or persona for the AI model. It's like whispering instructions to the AI about how it should behave.\n",
        " * `AIMessage (not imported here but commonly used)`: Represents a message from the AI.\n",
        " * `Significance`: These classes standardize how conversational turns are represented, making it easy for `LangChain` and `LangGraph` components to process and respond to them.\n",
        "\n",
        "# Decoding the System Message\n",
        " * `sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with using search and performing arithmetic on a set of inputs.\")`\n",
        "\n",
        "   * `sys_msg variable`: This simply stores an instance of a SystemMessage.\n",
        "\n",
        "   * `SystemMessage(...)`: We're creating a system message object.\n",
        "\n",
        "   * `content=\"...\":` This is the actual text of the system message.\n",
        "\n",
        "   * `\"You are a helpful assistant...\":` This sets the persona and tone of your AI agent. The LLM will try to adhere to this persona in its responses.\n",
        "   * `\"...tasked with using search...\"`: This explicitly tells the LLM that one of its primary responsibilities is to use a `search` tool. This primes the LLM to consider using the `search` tool you've bound to it whenever it encounters a query that might require external information.\n",
        "   * `\"...and performing arithmetic on a set of inputs.\"`: This instructs the LLM that it should also use its arithmetic tools `(add, multiply, divide)` when appropriate. This reinforces its ability and expectation to perform calculations.\n",
        "\n",
        "# Significance of the System Message:\n",
        "\n",
        "The system message is incredibly powerful in controlling the behavior and capabilities of your LLM agent. It acts as a high-level directive that guides the model's responses and decision-making process. By explicitly stating its role and the tools it should use, you significantly improve the chances that the LLM will correctly identify when to invoke `search, add, multiply, or divide` rather than trying to answer queries based solely on its internal knowledge (which, as we know, can sometimes be inaccurate for specific facts or calculations)."
      ],
      "metadata": {
        "id": "b2eKtu5Ju6lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# System message\n",
        "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with using search and performing arithmetic on a set of inputs.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "FlgpM-CFh3SH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's dive into this next piece of code, which defines a core `\"node\"` in your `LangGraph` agent. This is where the actual `\"thinking\"` or `\"reasoning\"` of your AI agent happens, powered by the LLM.\n",
        "\n",
        "# Understanding LangGraph Nodes\n",
        "In `LangGraph`, a node is a step or a function within your agent's workflow graph. Each node takes the current state of the application as input, performs some computation, and then returns an update to that state.\n",
        "\n",
        "## Breaking Down the reasoner Function\n",
        "1. `def reasoner(state: MessagesState)`:\n",
        "\n",
        " * `def reasoner(...)`: This defines a Python function named `reasoner`. In `LangGraph`, functions like this are often registered as nodes in your graph.\n",
        " * `MessagesState`: This is the input parameter to the `reasoner` function. `state` represents the current state of your agent's execution. The type hint `MessagesState` tells us that this specific node expects the state to be of the `MessagesState` type, which means it will contain a list of `BaseMessage` objects (like `HumanMessage, SystemMessage, AIMessage, ToolMessage`). When `LangGraph` calls this node, it automatically passes the current graph's state to it.\n",
        "2. `return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}`\n",
        "\n",
        "* This entire line is what the `reasoner node` does. It performs a computation and then returns an update to the graph's state.\n",
        "* `state[\"messages\"]:` This accesses the list of messages from the current graph state. Since we're using `MessagesState`, the state dictionary will have a key named `\"messages\"` that holds the conversation history.\n",
        "  * `[sys_msg] + state[\"messages\"]`:\n",
        "This is a crucial part. It constructs the complete list of messages that will be sent to the LLM.\n",
        "  * `[sys_msg]:` This is a list containing only your `SystemMessage` (\"You are a helpful assistant tasked with using search and performing arithmetic...\").\n",
        "  * `+ state[\"messages\"]`: This concatenates the `SystemMessage` with the existing conversation history from the state.\n",
        "  * `Significance`: By always including the `SystemMessage` at the beginning of the message list sent to the LLM, you ensure that the LLM consistently remembers its persona and instructions throughout the conversation. This prevents it from `\"forgetting\"` its role or the tools it has over multiple turns.\n",
        "\n",
        "* `llm_with_tools.invoke(...):`\n",
        "This is where your `gpt-4o` model instance (which is now \"aware\" of its tools) is actually called.\n",
        "It takes the combined list of messages `(system message + conversation history)` as input.\n",
        "The invoke method sends this message list to the OpenAI API.\n",
        "* `Expected Output:` Since `llm_with_tools` was `bind_tools`-ed, the `gpt-4o` model has two primary ways it can respond:\n",
        "  * `Natural Language (AIMessage)`: If it decides it has enough information to answer directly, it will return an `AIMessage` containing its textual response.\n",
        "  * `Tool Call (AIMessage with tool_calls)`: If it decides it needs to use one of the bound tools (like add, multiply, divide, or search), it will return an `AIMessage` that contains a `tool_calls` attribute. This `tool_calls` attribute will be a list of structured objects indicating which tool to call and with what arguments.\n",
        "\n",
        "* `{\"messages\": [...]}:` The `reasoner` function returns a dictionary. `LangGraph` expects nodes to return a dictionary where the keys correspond to the keys in your graph's state (in this case, `\"messages\"`). The values are then used to update the state. By returning `{\"messages\": [...]}` with the LLM's response inside, you are appending the LLM's latest `AIMessage` (whether it's a textual response or a tool call) to the existing messages list in the graph's state. LangGraph's `MessagesState` automatically handles this appending."
      ],
      "metadata": {
        "id": "MRVy0CTRxLf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nodes"
      ],
      "metadata": {
        "id": "H7z8wrV9550D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Node\n",
        "def reasoner(state: MessagesState):\n",
        "   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n"
      ],
      "metadata": {
        "id": "uJ6mJ1Hcio3U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the graph\n",
        "\n",
        "This block of code is where the magic of `LangGraph` really comes together to form an Agentic AI workflow. You're effectively building a simple `ReAct (Reason-Act)` agent here, which is a foundational pattern in the field.\n",
        "\n",
        "1. `Imports: Setting the Stage `\n",
        "* `from langgraph.graph import START, StateGraph`:\n",
        " * `StateGraph`: This is the fundamental class from `LangGraph ` used to define your state machine. You instantiate it with the type of state your graph will manage (`MessagesState` in our case, which manages a list of messages).\n",
        " * `START`: A special constant in `LangGraph` that represents the initial entry point into your graph. Any execution of the graph begins from START.\n",
        "* `from langgraph.prebuilt import tools_condition # this is the checker for the if you got a tool back:`\n",
        " * `tools_condition`: This is a pre-built utility function provided by `LangGraph`. Its purpose is to act as a conditional router. It inspects the output of the preceding node (which in our case is the reasoner's response from the LLM) and checks if that response contains a tool call.\n",
        "   * If the LLM's response indicates a tool call, tools_condition will return a specific string (often \"tools\") that tells the graph to transition to the tools node.\n",
        "   * If the LLM's response is a final answer (i.e., no tool call), `tools_condition` will return a string like `\"__end__\"` (representing the END state), causing the graph to stop.\n",
        "* `from langgraph.prebuilt import ToolNode`:\n",
        "  * `ToolNode`: This is another pre-built node provided by `LangGraph`. It's specifically designed to execute the tools that an LLM has decided to call.\n",
        "   * It takes a list of tool objects (like our `add, multiply, divide, search` functions) as its argument.\n",
        "   * When this node is activated, it looks at the tool calls generated by the LLM, finds the corresponding tool in its list, executes it, and returns the tool's output back into the graph's state (often as a `ToolMessage`).\n",
        "* `from IPython.display import Image, display`:\n",
        "These are utility functions from `IPython` (which Jupyter notebooks use) for displaying rich output, like images. We'll use this to visualize the graph structure.\n",
        "\n",
        "2. `Graph Initialization (builder = StateGraph(MessagesState))`\n",
        "* `builder = StateGraph(MessagesState)`: This creates an instance of `StateGraph`. We're telling `LangGraph` that our entire workflow will operate on a state that is structured like `MessagesState` (i.e., it will primarily manage a list of messages representing the conversation history).\n",
        "\n",
        "3. `Adding Nodes (builder.add_node(...))`\n",
        "* `builder.add_node(\"reasoner\", reasoner):`\n",
        " * This adds a node to our graph.\n",
        " * `\"reasoner\"`: This is the name we're giving to this node within our graph. It's a string identifier.\n",
        " * `reasoner`: This refers to the Python function reasoner that you defined earlier. This function encapsulates the logic where the LLM (`llm_with_tools`) receives the current state and generates a response (either a direct answer or a tool call). This is the `\"Reason\"` step in ReAct.\n",
        "\n",
        "* `builder.add_node(\"tools\", ToolNode(tools)):`\n",
        "\n",
        " * `\"tools\"`: The name for this node.\n",
        " * `ToolNode(tools)`: This creates an instance of the `ToolNode ` and passes it our tools list (`[add, multiply, divide, search]`). When this node is executed, it will run the actual tool requested by the LLM. This is the \"Act\" step in `ReAct`.\n",
        "\n",
        "4. `Adding Edges: Defining the Workflow Logic`\n",
        "Edges define how the execution flows between nodes in the graph.\n",
        "\n",
        "* `builder.add_edge(START, \"reasoner\"):`\n",
        "\n",
        "  * This adds a direct, unconditional edge from the START point of the graph to the `\"reasoner\"` node.\n",
        "  * `Meaning`: When the graph execution begins (e.g., when a new user query comes in), the first thing it does is go to the reasoner node to let the LLM think.\n",
        "\n",
        "* `builder.add_conditional_edges(\"reasoner\", tools_condition):`\n",
        "\n",
        "  * This is the most critical part for building an agent. It adds conditional transitions from the `\"reasoner\"` node.\n",
        "  * `\"reasoner\"`: The starting node for this conditional edge.\n",
        "  * `tools_condition`: This is the function that will be called. It takes the output of the `\"reasoner\"` node as input.\n",
        "* How it works:\n",
        "After the `reasoner` node finishes, `tools_condition` is invoked.\n",
        "  * If reasoner's output (the LLM's response) is an `AIMessage` that contains `tool_calls`, then `tools_condition` will return the name of the `\"tools\"` node (implicitly, usually `\"tools\"` by default for `ToolNode`). This means the graph will transition to the `\"tools\"` node to execute the requested tool.\n",
        "  * If reasoner's output is an `AIMessage` that is a final answer (no `tool_calls`), then `tools_condition` will return END (or __end__). This tells the graph that the agent has finished its task for this turn, and the execution stops.\n",
        "* `Meaning`: This effectively creates a decision point: Did the LLM decide it needs to use a tool, or did it provide a final answer?\n",
        "\n",
        "* `builder.add_edge(\"tools\", \"reasoner\"):`\n",
        "\n",
        "  * This adds a direct, unconditional edge from the `\"tools\" ` node back to the `\"reasoner\"` node.\n",
        "  * `Meaning`: After a tool has been executed (by the `\"tools\" ` node), the result of that tool execution (an `\"observation\"`) is added to the state. The graph then immediately transitions back to the `\"reasoner\"` node. This allows the LLM to observe the result of the tool call and then reason again, deciding if it needs another tool call, if it can now answer, or if it needs to try something else. This creates the `\"Reason-Act-Observe-Reason\"` loop of the `ReAct` pattern.\n",
        "5. `Compiling the Graph (react_graph = builder.compile())`\n",
        " * `react_graph = builder.compile()`:\n",
        "   * After you've defined all your nodes and edges using the builder object, you `compile()` the graph.\n",
        "   * Purpose: The `compile()` method performs several optimizations and checks. It essentially \"finalizes\" the graph structure, making it ready for execution. It also creates a runnable object (`react_graph`) that you can then invoke with your inputs to start the agent's workflow.\n",
        "\n",
        "6. `Displaying the Graph (display(Image(react_graph.get_graph(xray=True).draw_mermaid_png())))`\n",
        "\n",
        " * `react_graph.get_graph(xray=True):` This method generates an internal representation of your graph. xray=True often helps in visualizing more details about the graph's internal workings, including what conditions are being evaluated.\n",
        " * `.draw_mermaid_png():` This method takes the graph representation and generates a Mermaid.js diagram (a text-based diagramming tool) and then renders it as a PNG image.\n",
        " * `display(Image(...)):` This takes the generated PNG image and displays it directly in your Jupyter notebook output.\n",
        " * `Purpose:` This is incredibly useful for debugging and understanding your agent's control flow. You can visually inspect the paths your agent can take and confirm that your nodes and edges are connected as intended.\n",
        "\n",
        "# In a Nutshell: The ReAct Pattern\n",
        "\n",
        "This entire graph setup implements a fundamental `ReAct `(Reason-Act) agent pattern:\n",
        "\n",
        "1. `Reason (by reasoner node)`: The LLM looks at the current conversation (and system instructions/tools) and decides what to do next. It either:\n",
        "  * Provides a final answer.\n",
        "  * Decides to use a tool.\n",
        "  * `Act` (by tools node): If a tool is decided, the tools node executes it.\n",
        "  * `Observe` (implicitly through state update): The result of the tool is added back to the conversation state.\n",
        "  * Reason Again (loop back to reasoner): The LLM then sees the new observation and uses it to continue its thought process, potentially calling another tool or finally providing an answer.\n",
        "  * This loop continues until the LLM provides a final answer, at which point the `tools_condition` routes to END. This architecture allows for powerful, iterative problem-solving by the agent.\n",
        "\n"
      ],
      "metadata": {
        "id": "E5T9K_c1nNCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "from langgraph.prebuilt import tools_condition # this is the checker for the if you got a tool back\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Graph\n",
        "builder = StateGraph(MessagesState)\n",
        "\n",
        "# Add nodes\n",
        "builder.add_node(\"reasoner\", reasoner)\n",
        "builder.add_node(\"tools\", ToolNode(tools)) # for the tools\n",
        "\n",
        "# Add edges\n",
        "builder.add_edge(START, \"reasoner\")\n",
        "builder.add_conditional_edges(\n",
        "    \"reasoner\",\n",
        "    # If the latest message (result) from node reasoner is a tool call -> tools_condition routes to tools\n",
        "    # If the latest message (result) from node reasoner is a not a tool call -> tools_condition routes to END\n",
        "    tools_condition,\n",
        ")\n",
        "builder.add_edge(\"tools\", \"reasoner\")\n",
        "react_graph = builder.compile()\n",
        "\n",
        "# Display the graph\n",
        "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "o0FIm4j0io5S",
        "outputId": "0dc4280e-869a-45e8-f820-b4b9d7dda43c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here actually run your newly built LangGraph agent\n",
        "1. `messages = [HumanMessage(content=\"What is 2 times Brad Pitt's age?\")]`\n",
        " * `messages` variable: This line initializes a Python list named messages.\n",
        " * `[HumanMessage(content=\"What is 2 times Brad Pitt's age?\")]`: This creates a single element within the messages list.\n",
        "  * `HumanMessage(...)`: This is an instance of the `HumanMessage` class from `langchain_core.messages`. It represents a single message coming from a human user.\n",
        "  * `content=\"...\"`: This is the actual text of the user's query or instruction.\n",
        "**What this line does**: It sets up the initial input to your agent. This is the very first thing the user asks your AI assistant. It's a question that clearly requires both a search (to find Brad Pitt's age) and a mathematical operation (multiplication).\n",
        "\n",
        "2. `messages = react_graph.invoke({\"messages\": messages})`\n",
        "\n",
        " * `react_graph:` This refers to the compiled `LangGraph` object that you built in the previous step. It contains all the nodes (reasoner, tools) and edges (the flow logic) that define your agent's behavior.\n",
        " * `.invoke(...)`: This is the method used to execute a compiled `LangChain` Runnable or, in this case, a `LangGraph`. When you invoke a graph, you're kicking off its execution.\n",
        " * `{\"messages\": messages}`: This is the input to the invoke method.\n",
        "  * `LangGraph` expects its input to match the structure of the `StateGraph` it was initialized with. Since you built `builder = StateGraph(MessagesState)`, the graph expects a dictionary with a key named `\"messages\"`.\n",
        "The value associated with `\"messages\"` is the list of `HumanMessage` that you just created. This sets the initial state of the graph.\n",
        "\n",
        "# The full execution flow\n",
        "This single line triggers the entire agentic process you've defined:\n",
        "\n",
        "1. `Graph Start (START -> reasoner)`: The `react_graph` begins execution. The initial state `{\"messages\": [HumanMessage(...)]}` is passed to the reasoner node.\n",
        "2. `Reasoner's First Turn (reasoner node):`\n",
        "  * The reasoner function takes the state (which now contains `[HumanMessage(\"What is 2 times Brad Pitt's age?\")])`.\n",
        "  * It constructs the full prompt for the LLM: `[SystemMessage(...)] + [HumanMessage(...)]`.\n",
        "  * It calls `llm_with_tools.invoke()` with this prompt.\n",
        "  * The `gpt-4o` model receives the instruction and the user's question. It processes it. Since it needs Brad Pitt's age, and it knows about the search tool, it will likely generate a tool call in its response, something like: `tool_code(\"search\", {\"query\": \"Brad Pitt's age\"})`.\n",
        "  * The reasoner node then returns this `AIMessage` (which contains the tool call) to update the graph's state.\n",
        "\n",
        "3. `Conditional Edge (reasoner -> tools):`\n",
        "\n",
        "  * After the reasoner node completes, the `tools_condition` is evaluated.\n",
        "  * It sees that the `AIMessage` from the reasoner contains a `tool_calls` attribute. Therefore, `tools_condition` routes the execution to the \"tools\" node.\n",
        "4. Tools Node Execution (`tools node`):\n",
        "\n",
        "  * The ToolNode receives the `AIMessage` with the search tool call.\n",
        "  * It extracts the tool name (`search`) and arguments `(\"Brad Pitt's age\")`.\n",
        "  * It then executes the actual `DuckDuckGoSearchRun` instance with that query.\n",
        "  * `DuckDuckGo` performs the search and returns results (e.g., \"Brad Pitt is 60 years old as of Dec 18, 1963\").\n",
        "  * The `ToolNode` wraps this search result into a `ToolMessage` and adds it to the graph's state.\n",
        "5. Loop Back to `Reasoner (tools -> reasoner)`:\n",
        "\n",
        "  * After the tools node finishes, the graph unconditionally moves back to the \"reasoner\" node.\n",
        "6. Reasoner's Second Turn (reasoner node again):\n",
        "\n",
        "* The `reasoner` function now takes the updated state. This state contains:\n",
        "  * The original `SystemMessage`.\n",
        "  * The `HumanMessage` (\"What is 2 times Brad Pitt's age?\").\n",
        "  * The `AIMessage` from the first reasoner turn (containing the search tool call).\n",
        "  * The `ToolMessage` (containing the search result: \"Brad Pitt is 60 years old...\").\n",
        "* The `gpt-4o` model now sees the original question AND the result of the search.\n",
        "* It will then likely decide to use the multiply tool. It generates another tool call: tool_code(\"multiply\", {\"a\": 2, \"b\": 60}).\n",
        "* The reasoner returns this `AIMessage` to update the state.\n",
        "\n",
        "7. `Conditional Edge (reasoner -> tools)`:\n",
        "\n",
        " * `tools_condition` is evaluated again.\n",
        " * It sees the new `AIMessage` with the multiply tool call.\n",
        "Routes execution to the \"tools\" node again.\n",
        "\n",
        "8. Tools Node Execution (tools node again):\n",
        "\n",
        " * The `ToolNode` executes the multiply function with a=2 and b=60.\n",
        " * The multiply function returns 120.\n",
        "This result is wrapped into a ToolMessage and added to the state.\n",
        "9. Loop Back to Reasoner (`tools -> reasoner`):\n",
        "\n",
        "  * Unconditionally moves back to the reasoner node.\n",
        "10. Reasoner's Final Turn (reasoner node again):\n",
        "\n",
        " * The reasoner function takes the state, which now contains all previous messages and the `ToolMessage` with the 120 result.\n",
        " * The `gpt-4o` model sees the original question and the result of the multiplication. It now has all the information to answer directly.\n",
        " * It generates a final `AIMessage` with the answer: \"Brad Pitt is 60 years old, so 2 times his age is 120.\" (or something similar).\n",
        " * The reasoner returns this final `AIMessage`.\n",
        "11. `Conditional Edge (reasoner -> END):`\n",
        "\n",
        "* `tools_condition` is evaluated one last time.\n",
        "* It sees that the final `AIMessage` does not contain any `tool_calls.`\n",
        "Therefore, `tools_condition` routes the execution to END. The graph stops.\n",
        "\n",
        "12. `Return Value`: The `react_graph.invoke(...)` call returns the final state of the graph, which is a dictionary containing the complete list of messages `(all HumanMessage, SystemMessage, AIMessages, and ToolMessages)` that were exchanged during the execution. By assigning it back to the messages variable, you capture the entire conversation history and the agent's thought process.\n",
        "\n",
        "This single line of code orchestrates a sophisticated, multi-step reasoning and action process for your AI agent!\n"
      ],
      "metadata": {
        "id": "xcl-OFP9v3PA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content=\"What is 2 times Brad Pitt's age?\")]\n",
        "messages = react_graph.invoke({\"messages\": messages})"
      ],
      "metadata": {
        "id": "yYz5Wc9dio76"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What This Code Does\n",
        " 1. `for m in messages['messages']:`:\n",
        "\n",
        "  * `messages`: This refers to the variable where you stored the final state returned by `react_graph.invoke()`. Remember, when the graph finishes running, it returns a dictionary representing its final state.\n",
        "  * `messages['messages']`: Since your `StateGraph` was configured with `MessagesState`, the graph's state dictionary will have a key named `'messages'`. The value associated with this key is the complete list of all messages that were exchanged during the agent's entire execution `(System, Human, AI, and Tool messages, in chronological order)`.\n",
        "  * `for m in ...`: This initiates a simple Python for loop. It iterates through each individual `message (m)` in that ordered list of messages.\n",
        "\n",
        "2. `m.pretty_print()`:\n",
        "\n",
        " * `m`: In each iteration of the loop, m is an instance of a `LangChain BaseMessage` subclass (e.g., `HumanMessage, AIMessage, ToolMessage, SystemMessage`).\n",
        " * `.pretty_print()`: This is a convenient method provided by these `LangChain` message objects. Its purpose is to output the message content to the console in a formatted, readable way, often with different colors or prefixes to distinguish between Human, AI, System, and Tool messages.\n",
        "\n",
        "# Significance for Agentic AI\n",
        "This simple loop is incredibly important for *observability and debugging* your agent:\n",
        "\n",
        " * `Understanding Agent Thought Process`: When an agent performs multiple steps (like searching for Brad Pitt's age, then multiplying it), a direct `print(messages)` would give you a raw Python object. `pretty_print()` allows you to see the entire sequence of thoughts and actions taken by your agent:\n",
        "  * What was the initial human query?\n",
        "  * What tools did the AI decide to call?\n",
        "  * What were the results of those tool calls?\n",
        "  * What was the AI's final answer?\n",
        " * `Debugging`: If your agent isn't behaving as expected, `pretty_print()` provides a detailed trace. You can see exactly what prompt the LLM received at each step, what tool calls it attempted, and what observations it made. This helps you identify if:\n",
        "    * The LLM misinterpreted your instructions.\n",
        "    * The tool was called incorrectly.\n",
        "    * The tool returned an unexpected result.\n",
        "    * The LLM failed to incorporate the tool's result into its final answer.\n",
        "\n",
        "  * `User Experience (for developers)`: While not for end-users, for developers, this is a quick and effective way to review the agent's \"internal monologue\" and verify its execution flow.\n",
        "\n",
        "This code block takes the complex, multi-turn interaction that just happened inside your LangGraph agent and presents it in a clear, step-by-step format, allowing you to easily understand how your AI arrived at its conclusion."
      ],
      "metadata": {
        "id": "QD1vYt1UX-8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvzaROdwi64G",
        "outputId": "4049bf90-b2ea-481d-e035-55694bb8730a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is 2 times Brad Pitt's age?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  duckduckgo_search (call_lXfiZKEKGQX1RlF767QyXRfa)\n",
            " Call ID: call_lXfiZKEKGQX1RlF767QyXRfa\n",
            "  Args:\n",
            "    query: Brad Pitt age\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: duckduckgo_search\n",
            "\n",
            "Actor Brad Pitt is known the movies 'Fight Club,' 'The Curious Case of Benjamin Button,' and 'Moneyball.' Read about his age, girlfriend, children, and more. Brad Pitt is an American actor known for his portrayal of unconventional characters. He channeled his magnetism as a leading man in Legends of the Fall (1994) but avoided being typecast by playing offbeat characters. ... December 18, 1963, Shawnee, Oklahoma, U.S. (age 61) (Show more) Founder: Make It Right Not on Our Watch (Show more) Awards ... Brad Pitt: Net Worth, Age, Height & Everything You Need To Know About the Oscar-Winning Actor. By Samantha Crowell & Colin McCormick. Updated Jan 26, 2025. Follow Followed Like Thread Link copied to clipboard. Related. The Accountant 2 Box Office Hits Major Domestic Milestone & Climbs Ben Affleck's Highest-Grossing Charts ... Brad Pitt Age. Brad Pitt age 61 years old in 2024, He was born on December 18, 1963 in Shawnee, Oklahoma in the United States. Brad Pitt Height and Weight . Brad Pitt is an educated, attractive, and dashing boy with a lovely demeanor. He has a regular body type, outstanding physical measurements, and a powerful, handsome figure. Learn about Brad Pitt's net worth, age, height, weight, biography, family history, and more in this comprehensive guide. Find out his greatest paid film, favorite things, social media accounts, and frequently asked questions.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  multiply (call_zja5q7JiFVkJjHwjwdXtnbw0)\n",
            " Call ID: call_zja5q7JiFVkJjHwjwdXtnbw0\n",
            "  Args:\n",
            "    a: 61\n",
            "    b: 2\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: multiply\n",
            "\n",
            "122\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Brad Pitt is currently 61 years old. Therefore, 2 times his age is 122.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More manual way and adding a custom tool\n",
        "\n",
        "* `!pip -q install`\n",
        "\n",
        "  * `!`: This is a \"shell escape\" character used in Jupyter Notebooks or IPython environments (like Google Colab, which you've been using). It tells the environment to run the following command as a regular command-line instruction, not as Python code. If you were in a standard terminal, you'd just type pip.\n",
        "  * `pip`: This is Python's standard package installer. It's the primary tool used to install, upgrade, and manage software packages (libraries) written in Python from the Python Package Index (PyPI) or other package indexes.\n",
        "  * `-q`: This is a \"quiet\" flag. It tells pip to install the package without printing a lot of detailed progress information to your console. This keeps your output cleaner, showing only essential messages or errors.\n",
        "\n",
        "* `yahoo-finance`:\n",
        "\n",
        " * This is the name of the Python package you want to install.\n",
        " this library allows your Python code to:\n",
        "\n",
        "   * Access financial data from Yahoo Finance. This typically includes:\n",
        "   * Historical stock prices (open, high, low, close, volume).\n",
        "   * Current stock quotes.\n",
        "   * Company information.\n",
        "   * Other financial metrics."
      ],
      "metadata": {
        "id": "QtczHZq39HKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install yahoo-finance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KteveCcwHXsl",
        "outputId": "82f46bcd-8b8d-4c3a-e1ee-3b01ec82bb07"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for yahoo-finance (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a great step because now creating a custom tool for your Agentic AI to interact with real-world data (stock prices).\n",
        "\n",
        "1. `import yfinance as yf`\n",
        " * `yfinance`: This is a popular and very useful Python library that allows you to easily download historical and real-time market data from Yahoo Finance. Remember the `!pip install yahoo-finance` command you ran? This is the line that actually brings that installed library into your Python script.\n",
        " * `as yf`: This is just a common Python alias. Instead of typing `yfinance` every time you want to use something from the library, you can just type yf. It makes your code shorter and easier to read.\n",
        "\n",
        "`What it does`: This line makes all the functions and capabilities of the `yfinance` library available for you to use in your code.\n",
        "\n",
        "2. `def get_stock_price(ticker: str) -> float`:\n",
        "  * `def get_stock_price(...)`: This defines a new Python function named `get_stock_price`. This is the function that will become one of your AI agent's \"skills\" or \"tools.\"\n",
        "  * `ticker: str`: This specifies that the `get_stock_price`  function expects one input, named ticker, and that this input should be a string (text). A \"ticker\" is the short symbol used to identify a company's stock (e.g., \"AAPL\" for Apple, \"GOOG\" for Google, \"TCS.NS\" for TCS on NSE).\n",
        "  * `-> float`: This is a `\"type hint\"` for the return value. It tells us that this function is expected to return a floating-point number (a number with decimals), which makes sense for a stock price.\n",
        "\n",
        "3. `The Docstring (The AI's Instruction Manual)`\n",
        " * This block of text enclosed in triple quotes (\"\"\"...\"\"\") is called a docstring.\n",
        " * `Why it's crucial for Agentic AI`: When you `\"bind\"` this function as a tool to your Language Model (like `gpt-4o`), `LangChain` (and `LangGraph`) will read this docstring. It's like giving your AI agent a miniature instruction manual for this specific tool.\n",
        "   * The first line (\"Gets a stock price from Yahoo Finance.\") tells the AI what the tool does in simple language.\n",
        "   * `The Args`: section (\"ticker: ticker str\") tells the AI what inputs the tool needs and what kind of information those inputs are.\n",
        " * The Commented-Out Line `(# \"\"\"This is a tool for getting the price of a stock when passed a ticker symbol\"\"\")`: This is just a commented-out line. It's likely an older version of the docstring or a note from the developer. Only the uncommented docstring (the one above it) will be read by `LangChain` as the tool's description.\n",
        "\n",
        " **What the docstring does**: It acts as the descriptive metadata that the LLM uses to understand when to use this tool and how to call it (what arguments to provide).\n",
        "\n",
        " 4. The Function Body: How the Tool Works\n",
        " * `stock = yf.Ticker(ticker):`\n",
        "   * This is the first line of code that actually does something.\n",
        "   * `yf.Ticker(ticker)` creates a `Ticker` object from the yfinance library. You give it the ticker symbol (like \"AAPL\"), and this object then knows how to go fetch data for that specific stock.\n",
        "\n",
        " * `return stock.info['previousClose']:`\n",
        "   * `stock.info`: The `Ticker` object has an info attribute, which is a dictionary containing a lot of fundamental information about the stock (company name, industry, various financial metrics, etc.).\n",
        "   * `['previousClose']`: We are specifically accessing the value associated with the key `'previousClose'` from that info dictionary. This gives us the closing price of the stock from the previous trading day.\n",
        "   * `return`: This sends the retrieved stock price back as the output of the `get_stock_price` function.\n",
        "\n",
        "**What it does**: This part of the code makes an actual request to Yahoo Finance, retrieves information for the given stock ticker, and extracts the previous day's closing price.\n",
        "\n",
        "# Significance for Agentic AI (Bridging to your Agent)\n",
        "\n",
        "This `get_stock_price` function is a perfect example of a custom tool that your AI agent can now use:\n",
        "\n",
        " * `Providing a \"Skill\"`: Just like you taught your agent to add, multiply, divide, and search, you're now giving it the new `\"skill\"` to `get_stock_price`.\n",
        " * `LLM's Role`: When a user asks your agent a question like, `\"What was Apple's stock price yesterday?\"` or `\"Compare Google's stock price with Microsoft's today,\"` your `gpt-4o` model will:\n",
        "    * Read the query.\n",
        "    * Look at the descriptions of all the tools you've bound to it (including the `get_stock_price` docstring).\n",
        "    * `\"Decide\"` that `get_stock_price` is the right tool for `\"Apple's stock price.\"`\n",
        "    * Generate a `\"tool call\"` telling `LangGraph` to run `get_stock_price` with `ticker=\"AAPL\"`.\n",
        " * `Execution`: `LangGraph` intercepts that tool call, runs your actual Python function `get_stock_price(\"AAPL\")`.\n",
        " * `Observation`: The result (e.g., 175.25) is then sent back to the LLM as an \"observation.\"\n",
        " * Further Reasoning: The LLM now has this piece of real-world data and can use it to answer the user's question, perhaps combine it with other information, or even use another tool (like divide if the user asked for a ratio of stock prices)."
      ],
      "metadata": {
        "id": "0U_VgHeKdFiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "\n",
        "def get_stock_price(ticker: str) -> float:\n",
        "    \"\"\"Gets a stock price from Yahoo Finance.\n",
        "\n",
        "    Args:\n",
        "        ticker: ticker str\n",
        "    \"\"\"\n",
        "    # \"\"\"This is a tool for getting the price of a stock when passed a ticker symbol\"\"\"\n",
        "    stock = yf.Ticker(ticker)\n",
        "    return stock.info['previousClose']"
      ],
      "metadata": {
        "id": "0pMlZscoM6e_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_stock_price(\"AAPL\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvJ113s9Ljgq",
        "outputId": "1adc71aa-a37b-4284-811f-a647d25568b1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "203.92"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_community.tools.yahoo_finance_news import YahooFinanceNewsTool\n",
        "\n",
        "# y_finance = YahooFinanceNewsTool()\n",
        "\n",
        "# y_finance.invoke(\"AAPL\")"
      ],
      "metadata": {
        "id": "Nlv8Um_zBLac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What this reasoner function does (step-by-step):\n",
        "This function acts as the core `\"thinking\"` or `\"reasoning\"` node in your `LangGraph` agent. It's where your LLM (`gpt-4o`) receives the current situation, decides what to do, and generates a response.\n",
        "\n",
        "1. `def reasoner(state)`:\n",
        "\n",
        "* This defines the function `reasoner`. In `LangGraph`, this function will be called as a \"node\" in your graph.\n",
        "* `state`: This is the input to the node. It's a dictionary that holds the current \"memory\" or \"context\" of your agent's ongoing task. When LangGraph runs, it passes the current state to this node.\n",
        "2. `query = state[\"query\"]`\n",
        "\n",
        "* This line expects that the state dictionary now contains a key named `\"query\"`.\n",
        "* This `\"query\"` key is where the current user's prompt or question is stored.\n",
        "* `Significance`: This is a change from how we might have initialized the messages list directly in the previous invoke call. Now, the main user query for the current turn is explicitly passed in the state via the `\"query\"` key.\n",
        "\n",
        "3. `messages = state[\"messages\"]`\n",
        "\n",
        " * This line retrieves the existing conversation history from the state dictionary.\n",
        " * Remember, this messages list will contain all the previous interactions (system messages, previous human inputs, AI responses, tool calls, and tool outputs).\n",
        "\n",
        "4. `sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with using search, the yahoo finance tool and performing arithmetic on a set of inputs.\")`\n",
        "\n",
        "  * This line re-defines the `SystemMessage` every time the reasoner node runs.\n",
        "  * `Crucial Update`: The content of this `SystemMessage` has been updated to explicitly tell the LLM that it is \"tasked with using... the yahoo finance tool\". This is how your gpt-4o model knows that it now has the `get_stock_price` function available as a tool, in addition to search and arithmetic.\n",
        "  * Why define it here? While often defined once globally, defining it within the node ensures it's always included and can theoretically be made dynamic based on other parts of the state if needed (though it's static here).\n",
        "\n",
        "5. `message = HumanMessage(content=query)`\n",
        "This line takes the query (the current user input) that was extracted from the state and wraps it into a `HumanMessage` object.\n",
        "\n",
        "6. `messages.append(message)`\n",
        "\n",
        "This is a  important step. The newly created `HumanMessage` (containing the user's current question) is added to the messages list.\n",
        "`Significance`: This ensures that the LLM sees the latest user input as part of the overall conversation history that is about to be sent to it.\n",
        "\n",
        "7. `result = [llm_with_tools.invoke([sys_msg] + messages)]`\n",
        "\n",
        "This is the core communication with your `gpt-4o` model.\n",
        "\n",
        " * `[sys_msg] + messages`: This creates the complete prompt that is sent to the LLM for this turn. It starts with the `SystemMessage` (setting the persona and capabilities) and then adds the entire chronological list of messages (previous conversation + current user query).\n",
        " * `llm_with_tools.invoke(...)`: This sends the combined prompt to your gpt-4o model.\n",
        " * What gpt-4o returns:\n",
        "   * If it has a final answer, it returns an `AIMessage` with its textual response.\n",
        "   * If it decides it needs to use a tool (e.g., `search, add, multiply, divide, or now get_stock_price`), it returns an `AIMessage` that contains a `tool_calls` attribute, specifying which tool to call and with what arguments (e.g., `tool_calls=[ToolCall(name='get_stock_price', args={'ticker': 'AAPL'})]`).\n",
        "   * `result = [...]`: The LLM's response (`AIMessage`) is wrapped in a list and stored in the result variable.\n",
        "8. `return {\"messages\":result}`\n",
        "\n",
        "This line returns a dictionary to `LangGraph`.\n",
        "  * `{\"messages\": ...}`: This tells `LangGraph` to update the `\"messages\"` key in the graph's state.\n",
        "  * `result`: The result (which is the single `AIMessage` from the LLM's current turn) is used to update the messages list in the global graph state. LangGraph's `MessagesState` intelligently handles appending this new message to the existing list.\n",
        "\n",
        "**In Summary:** The `reasoner` node's updated function:\n",
        "This reasoner function is the brain of your agent. For every turn that your agent needs to think or respond, this node runs:\n",
        "\n",
        "  * It takes the latest user input and the entire conversation history from the current state.\n",
        "  * It combines these with a clear instruction (`SystemMessage`) to the `gpt-4o` model, explicitly telling it about all the tools it can use (search, arithmetic, and now Yahoo Finance).\n",
        "  * It sends this complete context to `gpt-4o`.\n",
        "  * It then takes `gpt-4o's` response (either a direct answer or a decision to use a tool) and updates the conversation history within the graph's state.\n",
        "\n",
        "This reasoner node, combined with the conditional logic you set up in your graph (`tools_condition`), enables the agent to iteratively `\"reason\"` (call the LLM), `\"act\"` (execute a tool if needed), and then `\"observe\"` the tool's output to `\"reason\"` again, forming a powerful cycle for solving complex problems.\n"
      ],
      "metadata": {
        "id": "HNahDEZ9yNmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Node\n",
        "def reasoner(state):\n",
        "    query = state[\"query\"]\n",
        "    messages = state[\"messages\"]\n",
        "    # System message\n",
        "    sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with using search, the yahoo finance tool and performing arithmetic on a set of inputs.\")\n",
        "    message = HumanMessage(content=query)\n",
        "    messages.append(message)\n",
        "    result = [llm_with_tools.invoke([sys_msg] + messages)]\n",
        "    return {\"messages\":result}\n"
      ],
      "metadata": {
        "id": "6-unsLIxCKOq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. tools = [add, multiply, divide, search, get_stock_price]\n",
        "\n",
        "  * `tools variable`: This line re-defines (or updates) the tools list that your agent has access to.\n",
        "  * `[add, multiply, divide, search, get_stock_price]`: You'll notice that the `get_stock_price` function (the one you just created that fetches stock prices from Yahoo Finance) has now been added to the end of this list.\n",
        "  * `add, multiply, divide`: Your arithmetic tools.\n",
        "  * `search`: Your web search tool (`DuckDuckGo`).\n",
        "  * `get_stock_price`: Your brand-new stock price fetching tool.\n",
        "\n",
        "`What this line does`: This is the explicit step where you tell your Agentic AI framework, \"Here's the complete set of all the external skills/functions my AI agent can use.\"\n",
        "\n",
        "2. `llm = ChatOpenAI(model=\"gpt-4o\")`\n",
        "  * `llm variable`: This line re-creates a fresh instance of your `ChatOpenAI` object, which is your connection to the gpt-4o model.\n",
        "  * **Why repeat this?**  You might wonder why you're creating `llm` again. In some setups, especially when you're iteratively building and testing, it's common to re-initialize the core LLM object to ensure you're working with a clean instance before binding new tools or changing configurations. It simply makes sure that the `llm` variable points to a brand-new connection to gpt-4o.\n",
        "  * **What this line does:** It ensures you have a clean, ready-to-use interface to the powerful gpt-4o language model from OpenAI.\n",
        "\n",
        "3. `llm_with_tools = llm.bind_tools(tools)`\n",
        "\n",
        "  * `llm_with_tools variable`: This line is creating a new version of your `llm` object, now \"aware\" of its capabilities.\n",
        "  * `llm.bind_tools(tools)`: This is the crucial step, similar to what we discussed before, but now with an expanded set of tools.\n",
        "  * `LangChain` takes the llm object and \"teaches\" it about each and every tool in the updated tools list.\n",
        "  * For each tool (`add, multiply, divide, search, get_stock_price`), `LangChain` extracts its structured description (from the docstrings and type hints, or the tool's internal definition for `DuckDuckGoSearchRun`).\n",
        "\n",
        "These descriptions are then used to inform the gpt-4o model. When you send a query to llm_with_tools later, LangChain will automatically inject these tool descriptions into the prompt sent to gpt-4o.\n",
        "\n",
        "**What this line does** : It updates your gpt-4o instance's \"understanding\" of what external actions it can perform. Now, if a user asks a question like \"What's the current price of TCS stock?\" the `gpt-4o` model will look at its internal instructions and the descriptions of all its tools, and realize that `get_stock_price` is the perfect tool for that job. It will then generate a request to call that tool.\n",
        "\n",
        "# Overall Significance\n",
        "This entire block of code is about upgrading your Agentic AI's capabilities. You've added a new, specialized skill `(get_stock_price)` to its toolbox. By re-binding the tools to your LLM, you ensure that the gpt-4o model is fully aware of all its available actions and can intelligently decide when to use each of them to respond to user queries or perform tasks.\n",
        "\n",
        "This is a fundamental part of building intelligent agents: progressively giving them more and more specific tools to interact with various data sources and APIs in the real world."
      ],
      "metadata": {
        "id": "OU3sK-8f5c-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [add, multiply, divide, search, get_stock_price]\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "llm_with_tools = llm.bind_tools(tools)"
      ],
      "metadata": {
        "id": "LBGXfOroEEVD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "Ak2n1HL1oS2u",
        "outputId": "236a4475-27ef-4083-d798-72a48599f4ff"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.get_stock_price(ticker: str) -> float>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>get_stock_price</b><br/>def get_stock_price(ticker: str) -&gt; float</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/&lt;ipython-input-17-dd2f630352cb&gt;</a>Gets a stock price from Yahoo Finance.\n",
              "\n",
              "Args:\n",
              "    ticker: ticker str</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are getting into the core of how LangGraph manages the \"memory\" and \"context\" of your AI agent! This piece of code defines the structure of the graph's state, which is essentially the shared memory or blackboard that all the nodes in your agent's workflow can read from and write to.\n",
        "\n",
        "1. Imports: Tools for Defining State\n",
        "\n",
        "* `from typing import Annotated, TypedDict`:\n",
        "\n",
        "  * `typing module`: This is a standard Python module that provides support for type hints.\n",
        "  * `TypedDict`: This is a special type from the typing module. It allows you to define a dictionary with a fixed set of keys, where each key has an associated type. It's essentially a way to create a schema or a \"typed dictionary\" for your data structure.\n",
        "  * `Significance for LangGraph`: `LangGraph` heavily relies on `TypedDict` to define the shape of its graph's state. This makes the state explicit, predictable, and easier to work with.\n",
        "  * `Annotated`: This is a powerful feature introduced in Python 3.9 (or 3.8 with from __future__ import annotations). It allows you to add metadata to a type hint.\n",
        "  * `Significance for LangGraph`: `LangGraph` uses Annotated to attach special instructions (like `operator.add`) to state fields, telling `LangGraph` how to combine updates to that field when multiple nodes modify it or when a node returns an update.\n",
        "* `import operator`:\n",
        "   * This built-in Python module provides functions that correspond to Python's operators (like +, -, *, ==).\n",
        "   * `operator.add`: This specifically corresponds to the + operator.\n",
        "   * `Significance for LangGraph`: When used with `Annotated`, `operator.add` tells `LangGraph`: \"If a node returns an update for this state field, use the + operator to combine the new value with the existing value, rather than overwriting the old value.\" This is crucial for appending to lists.\n",
        "\n",
        "* `from langchain_core.messages import AnyMessage`:\n",
        "\n",
        "   * `AnyMessage`: This is a type hint from langchain_core that represents any type of LangChain message. This includes `HumanMessage, AIMessage, SystemMessage, ToolMessage, FunctionMessage`, etc.\n",
        "   * `Significance`: It allows you to define a list that can contain all these different message types, which is exactly what a conversation history needs.\n",
        "\n",
        "* `from langgraph.graph.message import add_messages`:\n",
        "\n",
        "  * While imported here, `add_messages` is a utility function that `LangGraph` uses internally (or you could use manually) to handle the logic of appending messages.\n",
        "  * The Annotated[..., operator.add] on the messages field in `GraphState` effectively configures `LangGraph` to use this `\"add\"` logic automatically when updates come in for the messages list.\n",
        "\n",
        "2. `class GraphState(TypedDict)`: - Defining the Shared State Schema\n",
        "  * This block defines your custom `GraphState`. Any variable that represents the current state of your `LangGraph` execution will conform to this dictionary structure.\n",
        "\n",
        "  * \"\"\"State of the graph.\"\"\": A docstring explaining the purpose of this class.\n",
        "  * `query: str`:\n",
        "\n",
        "    * `Type: str (string)`.\n",
        "    * Purpose: This field is intended to hold the initial user input or the main query for the current run of your agent. Your reasoner function explicitly uses state[\"query\"]. This is a clean way to pass the initial user's request into the graph.\n",
        "  * `finance: str`:\n",
        "\n",
        "    * Type: str (string).\n",
        "    * Purpose: This is a new field! It's likely intended to store specific financial data or results that your agent fetches, perhaps using your get_stock_price tool. For example, after getting a stock price, a node might update state[\"finance\"] with that price or a formatted summary of it. This separates the specific financial data from the general conversation history.\n",
        " * `final_answer: str`:\n",
        "\n",
        "     * Type: str (string).\n",
        "     * `Purpose`: This is another new field, crucial for agents. It's intended to store the ultimate, consolidated answer that your agent formulates after all its reasoning and tool-use steps. This allows you to extract the final, human-friendly response easily at the end of the graph's execution, rather than sifting through all the intermediate messages.\n",
        " * `# intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]`:\n",
        "\n",
        "     * This line is commented out, meaning it's not active in your current graph state.\n",
        "     * `Purpose (if active)`: If uncommented, this would be a common pattern for tracking the \"thought process\" of a more traditional Agent in LangChain. AgentAction represents the agent's decision to use a tool, and str would be the tool's output (observation). operator.add would ensure that new action-observation pairs are appended to this list. It's useful for debugging agent behavior.\n",
        "\n",
        " * `messages: Annotated[list[AnyMessage], operator.add]`:\n",
        "\n",
        "     * `Type`: list[AnyMessage] (a list containing any type of LangChain message).\n",
        "     * `Annotated[..., operator.add]`: This is where Annotated and operator.add are vital.\n",
        "     * When a node (like your reasoner node) returns an update to the state that includes {\"messages\": [new_message]}, LangGraph sees this Annotated type with operator.add.\n",
        "     * Instead of replacing the entire messages list with just [new_message], LangGraph uses operator.add to append [new_message] to the existing messages list in the state.\n",
        "     * `Significance`: This is how your LangGraph automatically builds and maintains the entire conversation history. Every time a node processes something and generates a new message (AI response, tool call, tool observation), that message is seamlessly added to the messages list in the state, making the full context available to subsequent nodes (especially the reasoner).\n",
        "\n",
        "# Overall Purpose of GraphState\n",
        "This GraphState class acts as the central data model for your Agentic AI's workflow. It defines:\n",
        "\n",
        " * What information is available to each step (node) in your agent.\n",
        " * How that information is structured.\n",
        " * Crucially, how different updates from nodes are combined to form the evolving state (e.g., appending messages)."
      ],
      "metadata": {
        "id": "_zx1trmC_ldI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated, TypedDict\n",
        "import operator\n",
        "from langchain_core.messages import AnyMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"State of the graph.\"\"\"\n",
        "    query: str\n",
        "    finance: str\n",
        "    final_answer: str\n",
        "    # intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]\n",
        "    messages: Annotated[list[AnyMessage], operator.add]\n"
      ],
      "metadata": {
        "id": "RH1ap_Yuztf5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Review of Imports (Same as before):\n",
        "\n",
        "* `START, StateGraph` : Core `LangGraph` components for defining the entry point and the graph structure itself.\n",
        "* `tools_condition`: The pre-built function that checks if the LLM's response is a tool call or a final answer, deciding the next step.\n",
        "* `ToolNode`: The pre-built node that executes the actual tools chosen by the LLM.\n",
        "1. `Graph Initialization with Custom State (workflow = StateGraph(GraphState))`\n",
        "\n",
        "* `workflow = StateGraph(GraphState)`: This is the most significant change!\n",
        "Previously, you might have used `StateGraph(MessagesState)`.\n",
        "Now, you're explicitly telling `LangGraph` that the shared, mutable state that will be passed between all nodes in this workflow will conform to the structure defined by your `GraphState TypedDict`.\n",
        "* What this means: Your state object inside each node will now consistently have query, finance, final_answer, and messages fields, and the messages field will automatically handle appending new messages thanks to Annotated and operator.add. This provides a more robust and extensible way to manage your agent's context than just a simple list of messages.\n",
        "\n",
        "2. `Adding Nodes (workflow.add_node(...))`\n",
        "This part is conceptually the same as before, but it's important to understand what each node does within the context of your custom GraphState:\n",
        "\n",
        " * `workflow.add_node(\"reasoner\", reasoner)`:\n",
        "Adds your reasoner function as a node named \"reasoner\".\n",
        "    * `Role`: This is where your gpt-4o LLM, with all its tools, does its `\"thinking.\"` It takes the current state (which now includes query, finance, messages, etc.), and produces a response that's either a final answer or a tool call. The reasoner function expects and uses the state[\"query\"] and state[\"messages\"].\n",
        " * `workflow.add_node(\"tools\", ToolNode(tools))`:\n",
        "     * Adds a pre-built `ToolNode` as a node named `\"tools\"`. You pass it the tools list (which now includes `get_stock_price`).\n",
        "     * `Role`: If the reasoner decides to use a tool, execution moves here. This node will automatically look at the `tool_calls` generated by the LLM in the state, execute the correct tool, and add the tool's output back into the messages list in the state (as a `ToolMessage`).\n",
        "\n",
        "3. `Adding Edges: Defining the Agent's Flow`\n",
        "\n",
        "This section defines the flow of execution, creating the ReAct (Reason-Act) loop for your agent:\n",
        "\n",
        " * `workflow.add_edge(START, \"reasoner\")`:\n",
        "\n",
        "    * `Meaning`: The agent's process always begins by going to the reasoner node. This means the LLM gets the first opportunity to process the initial user query (which will be part of the state passed to the graph when you invoke it).\n",
        " * `workflow.add_conditional_edges(\"reasoner\", tools_condition)`:\n",
        "\n",
        "   * `Meaning`: After the reasoner node has finished its `\"thinking\"` (i.e., `llm_with_tools.invoke()` has returned its response), `LangGraph` uses `tools_condition` to decide the next * step: If the LLM's response (AIMessage) contains a tool_calls attribute (meaning it wants to use a tool), tools_condition will route the execution to the \"tools\" node.\n",
        "If the LLM's response is a final answer (no `tool_calls`), `tools_condition` will route the execution to the END of the graph, completing the current run.\n",
        "\n",
        "* `workflow.add_edge(\"tools\", \"reasoner\")`:\n",
        "\n",
        " * `Meaning`: After the \"tools\" node executes a tool and adds its output (the \"observation\") to the messages list in the state, the execution unconditionally moves back to the \"reasoner\" node.\n",
        " * `Why this loop`? This is fundamental to the ReAct pattern. It allows the LLM to observe the result of its action (the tool's output) and then reason again with this new information. It can then decide if it needs another tool call, if it can now answer the question, or if it needs to adjust its plan.\n",
        "\n",
        "4. `Compiling the Graph (react_graph = workflow.compile())`\n",
        " * `react_graph = workflow.compile()`: This step takes your defined nodes and edges and compiles them into an executable graph. This compilation optimizes the graph and makes it ready for you to invoke it with inputs. `react_graph` is now your fully assembled Agentic AI.\n",
        "\n",
        "5. `Displaying the Graph (display(Image(react_graph.get_graph(xray=True).draw_mermaid_png())))`\n",
        "This part uses IPython's display utilities to generate and show a visual diagram of your graph.\n",
        " * `react_graph.get_graph(xray=True).draw_mermaid_png()`: This creates a Mermaid.js diagram (and converts it to a PNG image) that visually represents the nodes and the flow of edges, including the conditional logic.\n",
        " * `Significance`: This is incredibly helpful for visualizing and debugging your agent's complex multi-step reasoning process. You can clearly see how the agent moves from reasoning to acting (using tools) and back to reasoning, eventually reaching a final answer or needing more action.\n",
        "\n",
        " In essence, this entire block of code is where you're bringing all your pieces together. You're defining the architecture and control flow of your AI agent, telling it explicitly what its internal memory looks like (GraphState), what its \"brain\" does (reasoner node), what \"actions\" it can take (tools node), and how it makes decisions to move between thinking and acting (conditional_edges). This is the blueprint for your complex, problem-solving AI agent!\n",
        "\n"
      ],
      "metadata": {
        "id": "5XzF4SdJNLMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "from langgraph.prebuilt import tools_condition # this is the checker for the\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "\n",
        "# Graph\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add Nodes\n",
        "workflow.add_node(\"reasoner\", reasoner)\n",
        "workflow.add_node(\"tools\", ToolNode(tools)) # for the tools\n",
        "\n",
        "# Add Edges\n",
        "workflow.add_edge(START, \"reasoner\")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"reasoner\",\n",
        "    # If the latest message (result) from node reasoner is a tool call -> tools_condition routes to tools\n",
        "    # If the latest message (result) from node reasoner is a not a tool call -> tools_condition routes to END\n",
        "    tools_condition,\n",
        ")\n",
        "workflow.add_edge(\"tools\", \"reasoner\")\n",
        "react_graph = workflow.compile()\n",
        "\n",
        "# Show\n",
        "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "1REv0ndD_Awn",
        "outputId": "e9243ca4-c583-4302-dddc-644f7ca54f7c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Full Agentic Flow Triggered by this Line (ReAct in Action):\n",
        "When you run `response = react_graph.invoke(...)`, here's a detailed recap of what happens, keeping the `GraphState` in mind:\n",
        "\n",
        "1. `Initial State`: The graph starts with `state = {\"query\": \"What is 2 times Brad Pitt's age?\", \"messages\": []}`.\n",
        "2. `START to reasoner`: Execution flows to the reasoner node.\n",
        "3. `reasoner (1st turn)`:\n",
        " * Reads `state[\"query\"]`.\n",
        " * Reads `state[\"messages\"]` (which is []).\n",
        " * Constructs the full message list for gpt-4o: `[SystemMessage(...), HumanMessage(\"What is 2 times Brad Pitt's age?\")]`.\n",
        " * `llm_with_tools.invoke()` is called. gpt-4o recognizes the need for Brad Pitt's age.\n",
        " * gpt-4o responds with an AIMessage containing a tool call for search (e.g., search(\"Brad Pitt's age\")).\n",
        " * The reasoner returns {\"messages\": [AIMessage_with_search_tool_call]}.\n",
        " * State Update: The AIMessage_with_search_tool_call is appended to state[\"messages\"].\n",
        "\n",
        "4. `reasoner -> tools_condition -> tools`: tools_condition sees the tool call and routes to the \"tools\" node.\n",
        "\n",
        "5. `tools (1st turn)`:\n",
        "  * Executes the search tool with the query \"Brad Pitt's age\".\n",
        "  * Receives the search result (e.g., \"Brad Pitt is 60 years old\").\n",
        "  * Creates a ToolMessage with this result.\n",
        "  * The ToolMessage is appended to state[\"messages\"].\n",
        "\n",
        "6. `tools -> reasoner`: Execution loops back to the reasoner node.\n",
        "7. reasoner (2nd turn):\n",
        "\n",
        "  * Receives the updated state (now including the search result in messages).\n",
        "  * Sends the entire history (SystemMessage, HumanMessage, AIMessage_with_search_tool_call, ToolMessage_with_search_result) to gpt-4o.\n",
        "  * gpt-4o processes this. It sees the age and recognizes the \"2 times\" part.\n",
        "  * gpt-4o responds with an AIMessage containing a tool call for multiply (e.g., multiply(a=2, b=60)).\n",
        "  * The reasoner returns {\"messages\": [AIMessage_with_multiply_tool_call]}.\n",
        "State Update: The AIMessage_with_multiply_tool_call is appended to state[\"messages\"].\n",
        "\n",
        "8. `reasoner -> tools_condition -> tools`: tools_condition sees the tool call and routes to the \"tools\" node.\n",
        "\n",
        "9. `tools (2nd turn)`:\n",
        "  * Executes the multiply tool with a=2, b=60.\n",
        "  * Receives the result 120.\n",
        "  * Creates a ToolMessage with this result.\n",
        "  * The ToolMessage is appended to state[\"messages\"].\n",
        "10. `tools -> reasoner`: Execution loops back to the reasoner node.\n",
        "11. `reasoner (3rd turn)`:\n",
        "  * Receives the fully updated state (now including the multiplication result).\n",
        "  * Sends the entire history to gpt-4o.\n",
        "  * gpt-4o now has all the information. It formulates the final answer.\n",
        "  * gpt-4o responds with an AIMessage containing the final textual answer (e.g., \"Brad Pitt is 60 years old, so 2 times his age is 120.\"). Crucially, this AIMessage does not contain tool_calls.\n",
        "  * The reasoner returns {\"messages\": [AIMessage_with_final_answer]}.\n",
        "State Update: The AIMessage_with_final_answer is appended to state[\"messages\"].\n",
        "\n",
        "12. reasoner -> tools_condition -> END: tools_condition sees no tool calls and routes to END. The graph execution finishes.\n",
        "13. Return Value: The response variable now holds the final GraphState dictionary, with messages containing the entire detailed interaction log.\n",
        "\n",
        "This single invoke call beautifully demonstrates the power of LangGraph to manage complex, multi-step agentic workflows by leveraging a clearly defined, evolving state!"
      ],
      "metadata": {
        "id": "2jIyk5ZwDGxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = react_graph.invoke({\"query\": \"What is 2 times Brad Pitt's age?\", \"messages\": []})"
      ],
      "metadata": {
        "id": "Zd_qJiEmDHjf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response['messages'][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrXHHgZSENOu",
        "outputId": "ee2551f5-fd95-42a2-92c7-32c977b0c254"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Brad Pitt is currently 61 years old. Therefore, 2 times his age is 122 years.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = react_graph.invoke({\"query\": \"What is the stock price of TCS?\", \"messages\": []})"
      ],
      "metadata": {
        "id": "EpBUkfSILf98"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in response['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "id": "w3fZeZGXLf-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbd8bc39-cfef-4cc9-ee86-cf038be3f78d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is the stock price of TCS?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_stock_price (call_xbnk6zEawm2daBMK8GmBf0CO)\n",
            " Call ID: call_xbnk6zEawm2daBMK8GmBf0CO\n",
            "  Args:\n",
            "    ticker: TCS\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_stock_price\n",
            "\n",
            "Error: KeyError('previousClose')\n",
            " Please fix your mistakes.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is the stock price of TCS?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_stock_price (call_xeiURc858TOO5Sp7MEIoZYhF)\n",
            " Call ID: call_xeiURc858TOO5Sp7MEIoZYhF\n",
            "  Args:\n",
            "    ticker: TCS\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_stock_price\n",
            "\n",
            "Error: KeyError('previousClose')\n",
            " Please fix your mistakes.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is the stock price of TCS?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_stock_price (call_JZNDLEbL252TEML9yXYclOW3)\n",
            " Call ID: call_JZNDLEbL252TEML9yXYclOW3\n",
            "  Args:\n",
            "    ticker: TCS.NS\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_stock_price\n",
            "\n",
            "3421.9\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is the stock price of TCS?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The stock price of TCS (Tata Consultancy Services) is ₹3421.90.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = react_graph.invoke({\"query\": \"What is the stock price of the company that Lip-Bu-Tan is CEO of?\", \"messages\": []})"
      ],
      "metadata": {
        "id": "c21ouY0PIQh-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in response['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szgNFSrzJQ0o",
        "outputId": "dd85dcc3-bc0d-467e-f05d-e0d429c1e3ce"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is the stock price of the company that Lip-Bu-Tan is CEO of?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  duckduckgo_search (call_vOKnp9bx6Oo7AV6s7ERI3hyA)\n",
            " Call ID: call_vOKnp9bx6Oo7AV6s7ERI3hyA\n",
            "  Args:\n",
            "    query: Lip-Bu Tan CEO company\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: duckduckgo_search\n",
            "\n",
            "A message from Intel CEO Lip-Bu Tan to all company employees. April 24, 2025. Follow Intel Newsroom on Social Media: About Intel. Intel (Nasdaq: INTC) is an industry leader, creating world-changing technology that enables global progress and enriches lives. Inspired by Moore's Law, we continuously work to advance the design and manufacturing ... Frank D. Yeary, who took on the role of interim executive chair of the board during the search for a new CEO, will revert to being the independent chair of the board upon Tan becoming CEO. \"Lip-Bu is an exceptional leader whose technology industry expertise, deep relationships across the product and foundry ecosystems, and proven track record ... Lip-Bu Tan will take the helm of Intel on March 18. Courtesy Intel. Intel's stock is soaring after Lip-Bu Tan, a respected veteran in the semiconductor industry, was chosen as its new CEO.The ... March 12 (Reuters) - Intel (INTC.O), opens new tab tapped former board member Lip-Bu Tan as its CEO on Wednesday, as the struggling American chipmaking icon attempts to emerge from one of its ... A little over two months into his new gig running Intel, CEO Lip-Bu Tan is laying out his plans to turn Intel into a legitimate chip manufacturer.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is the stock price of the company that Lip-Bu-Tan is CEO of?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_stock_price (call_oARjazZ257Eb33rG8OxWDQND)\n",
            " Call ID: call_oARjazZ257Eb33rG8OxWDQND\n",
            "  Args:\n",
            "    ticker: INTC\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_stock_price\n",
            "\n",
            "20.06\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is the stock price of the company that Lip-Bu-Tan is CEO of?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The stock price of Intel (ticker: INTC), the company that Lip-Bu Tan is CEO of, is $20.06.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = react_graph.invoke({\"query\": \"What will be the price of INTEL stock if it doubles?\", \"messages\": []})"
      ],
      "metadata": {
        "id": "MiJ7ooR0qfwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in response['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8aded62-fe1a-4747-f1c2-905b1d2dc060",
        "id": "-pVdTUUMqfwn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What will be the price of nvidia stock if it doubles?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_stock_price (call_8iPPMM261Ddgr4LdAlhQ4wW9)\n",
            " Call ID: call_8iPPMM261Ddgr4LdAlhQ4wW9\n",
            "  Args:\n",
            "    ticker: NVDA\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_stock_price\n",
            "\n",
            "119.1\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What will be the price of nvidia stock if it doubles?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  multiply (call_uiTbfZ1pyR4TGayOPHXT1vZB)\n",
            " Call ID: call_uiTbfZ1pyR4TGayOPHXT1vZB\n",
            "  Args:\n",
            "    a: 11910\n",
            "    b: 2\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: multiply\n",
            "\n",
            "23820\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What will be the price of nvidia stock if it doubles?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "If Nvidia's stock price doubles, it will be $238.20.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "hhs6PnuLMSvW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "a78f4d9d-0e1b-4ef7-c299-0f58b2c70490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LA3LBsu3T2Ag"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}