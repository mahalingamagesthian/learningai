{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2937d7a7-a68c-4781-a7b6-7b7187f61e58",
   "metadata": {},
   "source": [
    "when you download foundation models (like those from Hugging Face Transformers), you often see a model.json file (or sometimes config.json which serves a similar purpose). This file is crucial; it essentially contains the configuration of the model architecture. \n",
    "\n",
    "# Significance of model.json (or config.json): #\n",
    "\n",
    "`Model Architecture Definition: ` It specifies the layers, their types (e.g., attention, feed-forward), their sizes, and how they are connected. Without this file, the library wouldn't know the structure of the model it's supposed to load.\n",
    "\n",
    "`Hyperparameters: ` It includes the hyperparameters used to define the model's architecture, such as the number of attention heads, the hidden dimension size, the number of layers, dropout rates, and more.\n",
    "\n",
    "`Task-Specific Configuration: ` Sometimes, it might contain information specific to the task the model was trained for (though this is often in a separate tokenizer_config.json or inferred from the model type). \n",
    "\n",
    "`Enables AutoModel Functionality: ` Libraries like Hugging Face Transformers use this config.json (or model.json) to automatically determine the correct model class to instantiate using functions like AutoModel, AutoModelForSequenceClassification, AutoModelForCausalLM, etc. The library reads the config.json to understand the model's type and then loads the appropriate class.\n",
    "\n",
    "# Properties Typically Present in model.json (or config.json): #\n",
    "\n",
    "The exact properties will vary depending on the specific model architecture (e.g., Bidirectional Encoder Representation of Transformer - BERT, Generative pre-defined Transformer - GPT, Text-To-Text-Transfer-Transformer - T5). However, you'll commonly find fields like:\n",
    "\n",
    "`architectures: ` A list indicating the model's class or architecture name (e.g., ['T5ForConditionalGeneration']). This is what AutoModel uses to decide which class to load.\n",
    "\n",
    "`model_type: ` A string specifying the type of the model (e.g., \"t5\").\n",
    "\n",
    "# Dimensionality Parameters: #\n",
    "\n",
    "`hidden_size or d_model: ` The size of the hidden layers and embeddings.\n",
    "`num_attention_heads or n_head: ` The number of attention heads in the multi-head attention mechanisms.\n",
    "`num_hidden_layers or n_layer: ` The number of layers in the encoder and/or decoder.\n",
    "\n",
    "# Vocabulary Size: #\n",
    "\n",
    "`vocab_size: ` The number of tokens in the model's vocabulary.\n",
    "\n",
    "# Dropout Probabilities: #\n",
    "\n",
    "`dropout_rate, attention_dropout_rate: ` The dropout probabilities used during training.\n",
    "\n",
    "# Activation Functions: #\n",
    "\n",
    "`activation_function: ` The activation function used in the feed-forward layers (e.g., \"relu\", \"gelu\").\n",
    "\n",
    "# Maximum Sequence Length: #\n",
    "\n",
    "`max_position_embeddings: ` The maximum length of input sequences the model was trained to handle.\n",
    "\n",
    "# Specific Architecture Parameters: #\n",
    "\n",
    "Fields unique to the model's design, such as:\n",
    "For T5: is_decoder, is_encoder_decoder, tie_word_embeddings.\n",
    "For BERT: type_vocab_size, initializer_range.\n",
    "For GPT: n_embd, n_positions, n_ctx.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef1fe73-5f82-480e-8747-1d4fda68b6cc",
   "metadata": {},
   "source": [
    "As you can see, this config.json for google/flan-t5-large specifies the architecture (T5ForConditionalGeneration), various dimensionalities (d_model, d_ff, d_kv), the number of layers and heads, dropout, and even some task-specific parameters.\n",
    "\n",
    "In summary,The configuration files are available in the location, ~/.cache/huggingface/hub/models--google--flan-t5-large/snapshots/0613663d0d48ea86ba8cb3d7a44f0f65dc596a2a. the model.json (or config.json) is the metadata that defines the model's structure and how it should be built. It's essential for loading and using pre-trained models correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed2a72-271b-4da9-8519-b69692654c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.json\n",
    "{\n",
    "  \"architectures\": [\n",
    "    \"T5ForConditionalGeneration\"\n",
    "  ],\n",
    "  \"d_ff\": 2816,\n",
    "  \"d_kv\": 64,\n",
    "  \"d_model\": 1024,\n",
    "  \"decoder_start_token_id\": 0,\n",
    "  \"dropout_rate\": 0.1,\n",
    "  \"eos_token_id\": 1,\n",
    "  \"feed_forward_proj\": \"gated-gelu\",\n",
    "  \"initializer_factor\": 1.0,\n",
    "  \"is_encoder_decoder\": true,\n",
    "  \"layer_norm_epsilon\": 1e-06,\n",
    "  \"model_type\": \"t5\",\n",
    "  \"n_positions\": 512,\n",
    "  \"num_decoder_layers\": 24,\n",
    "  \"num_heads\": 16,\n",
    "  \"num_layers\": 24,\n",
    "  \"output_past\": true,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"relative_attention_max_distance\": 128,\n",
    "  \"relative_attention_num_buckets\": 32,\n",
    "  \"tie_word_embeddings\": false,\n",
    "  \"transformers_version\": \"4.23.1\",\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 32128\n",
    "}\n",
    "# generation_config.json \n",
    "{\n",
    "  \"_from_model_config\": true,\n",
    "  \"decoder_start_token_id\": 0,\n",
    "  \"eos_token_id\": 1,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"transformers_version\": \"4.27.0.dev0\"\n",
    "}\n",
    "# tokenizer_config.json        \n",
    "{\n",
    "  \"additional_special_tokens\": [\n",
    "  \"eos_token\": \"</s>\",\n",
    "  \"extra_ids\": 100,\n",
    "  \"model_max_length\": 512,\n",
    "  \"name_or_path\": \"google/t5-v1_1-large\",\n",
    "  \"pad_token\": \"<pad>\",\n",
    "  \"sp_model_kwargs\": {},\n",
    "  \"special_tokens_map_file\": \"/home/younes_huggingface_co/.cache/huggingface/hub/models--google--t5-v1_1-large/snapshots/314bc112b191ec17b625ba81438dc73d6c23659d/special_tokens_map.json\",\n",
    "  \"tokenizer_class\": \"T5Tokenizer\",\n",
    "  \"unk_token\": \"<unk>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550b14da-aea4-4ab5-aaba-753a1e2f04f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
