{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlT3OENe2kbi7S8WcGlwWT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahalingamagesthian/learningai/blob/main/8_AgenticRAG_WebDocument.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overall Flow of Your RAG Agent (The Robot's Day):\n",
        "  1. `Start`\n",
        "  2. **Decide** `(generate_query_or_respond):` The robot looks at the user's request and decides: \"Do I need to look something up (using `retrieve_blog_posts`) or can I just answer directly?\"\n",
        "     * If **Direct Answer**: The robot gives the answer and stops.\n",
        "     * If **Retrieve**: The robot proceeds to search.\n",
        "  3. **Search** `(retrieve)`: The robot uses its `retrieve_blog_posts` skill to search your knowledge base (Lilian Weng's blog posts).\n",
        "  4. **Grade** `(grade_documents)`: The robot looks at the search results and decides: \"Are these results actually useful and relevant to the user's question?\"\n",
        "     * If **Relevant**: The robot proceeds to generate an answer.\n",
        "     * If **Not Relevant**: The robot realizes its search wasn't good. It then tries to rephrase the original question to make it better.\n",
        "  5. **Rewrite Question** (`rewrite_question`): The robot rephrases the question.\n",
        "  6. **Loop Back:** The robot goes back to the `\"Decide\"` step (step 2), but now with the new, refined question in its memory. It will try searching again with the better question.\n",
        "  7. **Generate Answer** (`generate_answer`): If the search results were relevant, the robot uses them to formulate a concise answer.\n",
        "  8. **End**: The robot has provided its answer and is done for this turn.\n",
        "\n",
        "  This sophisticated graph allows your RAG agent to be dynamic, self-correcting, and resilient, ensuring it provides accurate answers by intelligently leveraging its knowledge base!\n",
        "\n",
        "# Core Concepts First:\n",
        "  * `Graph`: A collection of nodes (steps/functions) and edges (transitions) that define a workflow.\n",
        "  * `State`: The shared \"memory\" that gets passed from one node to the next. In your case, it's `MessagesState`, meaning the core shared memory is a list of conversational messages.\n",
        "  * `Node`: A single step in the workflow. It's a Python function (like `generate_query_or_respond`, `rewrite_question`, `generate_answer`) or a pre-built `LangGraph` component (like `ToolNode`). A node takes the current state as input and returns a new state (or a decision for the next edge).\n",
        "  * `Edge`: A connection between two nodes. It dictates the flow of execution from one step to the next.\n",
        "  * `START / END:` Special nodes that mark the entry and exit points of your graph.\n",
        "\n",
        "RAG agents combine the reasoning power of LLMs with access to external, up-to-date, and domain-specific information, overcoming the LLM's knowledge cutoff and hallucination tendencies.\n",
        "\n",
        "1. `pip install -U --quiet`\n",
        "* `pip`: this is Python's package installer.\n",
        "* `-U (or --upgrade)`: This flag tells pip to upgrade any existing packages to their newest available version. This is good practice to ensure you're using the latest features and bug fixes.\n",
        "* `--quiet (or -q)`: this suppresses most of the installation output, keeping your console clean.\n",
        "\n",
        "2. `langgraph`\n",
        "* `Role in RAG`: While RAG can be implemented with simpler `LangChain` chains, langgraph remains highly relevant for RAG agents.\n",
        "  * A simple RAG pipeline might be: `retrieve -> generate.`\n",
        "  * A RAG agent often involves more complex steps: `user_query -> decide_to_retrieve -> retrieve -> decide_to_generate -> generate -> decide_to_refine -> refine -> final_answer`. `langgraph` provides the state management and conditional logic to orchestrate such multi-step, iterative RAG processes, just like it did for your previous agents.\n",
        "  * It allows for **iterative refinement** (e.g., `retrieve, see if answer is good, if not, reformulate query and retrieve again`).\n",
        "\n",
        "3. `\"langchain[openai]\"`\n",
        "* `langchain`: This refers to the core `LangChain` library. It provides the framework for building LLM applications, including components for `RAG (like chains, prompt templates, output parsers, and interfaces for LLMs)`.\n",
        " * `[openai] (the \"extra\")`: This is a crucial part! When you see [something] after a package name in pip install, it means you're installing the base package along with optional dependencies required for a specific integration.\n",
        " * In this case, `langchain[openai]` ensures that all the necessary sub-packages and libraries for interacting with `OpenAI's` models (like `gpt-4o` for text generation and `text-embedding-ada-002` for embeddings) are installed. This typically includes `langchain-openai` and `openai` itself.\n",
        " * `Role in RAG`: This provides the `\"Generation\"` part of RAG (using an LLM to formulate an answer) and also often the `\"Embedding\"` part (converting text into numerical vectors for similarity search).\n",
        "\n",
        "4. `langchain-community`\n",
        "* `Role in RAG`: This library is a workhorse for RAG. It houses a vast collection of integrations for various services and data sources:\n",
        "* `Document Loaders`: Tools to load data from different formats and sources (PDFs, websites, databases, Markdown files, etc.). This is the initial `\"Retrieval\" setup` step – getting your data into the system.\n",
        "* `Vector Stores`: Interfaces to connect with various vector databases `(e.g., Chroma, FAISS, Pinecone, Qdrant)` where you store your embedded data chunks. This is central to efficient `\"Retrieval\"`.\n",
        "* `Embeddings`: Implementations for different embedding models (though OpenAI's are typically installed via `langchain[openai], langchain-community has others)`. Embeddings are the numerical representations of text that allow you to find semantically similar content.\n",
        "* `Utilities & Tools`: General-purpose tools that an agent might need, including those for RAG.\n",
        "\n",
        "5. `langchain-text-splitters`\n",
        "* `Role in RAG`: **This is absolutely critical and specific to RAG**.\n",
        "   * `Why text splitting?` Large documents (like a book, a long PDF, or a comprehensive knowledge base) are often too big to fit into the LLM's context window (the amount of text an LLM can process at once). Also, sending an entire document to an LLM for every query is inefficient and expensive.\n",
        "   * `What it does`: langchain-text-splitters provides various algorithms to break down large texts into smaller, manageable \"chunks\" or \"segments.\"\n",
        "* `How it works in RAG`:\n",
        "  * You load your raw documents using a `DocumentLoader` from `langchain-community`.\n",
        "  * You pass these documents to a `TextSplitter` from `langchain-text-splitters`. This splitter intelligently divides the text while trying to maintain semantic coherence (e.g., not splitting a sentence in half, respecting paragraphs or headings).\n",
        "  * These smaller chunks are then converted into numerical embeddings and stored in a vector store.\n",
        "  * When a user asks a question, the query is also embedded. The vector store is then searched to find the most `\"semantically similar\"` chunks from your knowledge base.\n",
        "  * Only these relevant, smaller chunks are then sent to the LLM (along with the user's query) for generation.\n",
        "\n",
        "**In Summary for RAG:**\n",
        "\n",
        "This pip install command sets up the core components for a robust RAG system:\n",
        "\n",
        " * `langchain-community`: For loading your data and storing your processed data (in vector stores).\n",
        " * `langchain-text-splitters`: For breaking down your large documents into manageable, retrievable chunks.\n",
        " * `langchain[openai]`: For creating embeddings of your chunks (so they can be searched) and for generating the final answer using an LLM.\n",
        " * `langgraph`: For orchestrating complex RAG agents that might involve multiple retrieval steps, iterative refinement, or conditional logic based on retrieval results.\n",
        " * You're now equipped with the fundamental tools to build agents that don't just \"reason\" but can also \"read\" from their own dedicated knowledge bases!"
      ],
      "metadata": {
        "id": "QfWEB3C43IJx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clj9vmHuz4j5",
        "outputId": "5b8d1d55-946f-4af5-9f5a-3d9b246ec3e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install -U --quiet langgraph \"langchain[openai]\" langchain-community langchain-text-splitters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Securely Setting Environment Variables\n",
        "\n",
        "This code snippet is a common and recommended way to prompt for and set environment variables in an interactive Python session (like a Jupyter Notebook or Google Colab). This is crucial for keeping sensitive information, such as your OpenAI API key, out of your direct code.\n",
        "\n",
        "# Why this is important for your RAG Agentic AI setup:\n",
        " * `Security`: Your OpenAI API key grants access to your OpenAI account and can incur costs. Hardcoding it directly into your notebook makes it vulnerable if you share the notebook, push it to public repositories (like GitHub), or even accidentally expose your screen. Using environment variables keeps it out of your visible code.\n",
        " * `Best Practice`: This is a widely adopted best practice in software development for managing sensitive credentials. Libraries like `LangChain` and openai are designed to automatically look for API keys in standard environment variables `(like OPENAI_API_KEY)`, so you don't have to explicitly pass the key around in every function call.\n",
        " * `Flexibility`: You can change your API key easily by setting the environment variable in your shell or in `Google Colab's \"Secrets\"` feature, without modifying your Python code.\n",
        "\n",
        "When you run this cell, a prompt will appear (often below the cell, sometimes requiring you to click on an input box). Enter your OpenAI API key there, and it will be securely set for your current session."
      ],
      "metadata": {
        "id": "SyJoP2Paud8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "\n",
        "def _set_env(key: str):\n",
        "    if key not in os.environ:\n",
        "        os.environ[key] = getpass.getpass(f\"{key}:\")\n",
        "\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujaeo13h0Cfj",
        "outputId": "ddb96bb7-89e9-419d-9a96-99e881162a2e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `from langchain_community.document_loaders import WebBaseLoader`\n",
        "\n",
        " * `langchain_community.document_loaders:` This is a module within the `langchain-community` package (which you just installed). As its name suggests, `document_loaders` provides tools for loading data from various sources and formats into a structured format that `LangChain` can easily work with. This is your gateway for getting external data into your RAG pipeline.\n",
        " * `WebBaseLoader`: This is a specific class within `document_loaders`.\n",
        " * `Purpose:` Its job is to fetch the content of a web page (from a given URL) and convert it into a Document object (or a list of Document objects) that `LangChain` understands. It's intelligent enough to extract the main readable content, stripping away navigation, ads, and other irrelevant HTML elements.\n",
        "\n",
        "2. `urls = [...]`\n",
        " * This is a standard Python list named urls.\n",
        " * It contains three strings, each of which is a URL (Uniform Resource Locator) pointing to a specific blog post by Lilian Weng (a well-known researcher in AI).\n",
        " * `Significance`: This list represents the sources from which you want to gather information for your RAG agent's knowledge base.\n",
        "\n",
        "3. `docs = [WebBaseLoader(url).load() for url in urls]`\n",
        "* This is a very common and efficient Python construct called a list comprehension. It's a concise way to create a new list by applying an operation to each item in an existing iterable (in this case, your urls list).\n",
        "\n",
        "* `Let's break down what happens for each url in the urls list:`\n",
        "    1. `. WebBaseLoader(url)`: For each URL (e.g., \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"), a new instance of the WebBaseLoader is created, configured to load that specific web page.\n",
        "    2. `.load()`: The `load()` method is then called on that `WebBaseLoader` instance. This is the action that:\n",
        "      * Sends an HTTP request to the URL.\n",
        "      * Downloads the HTML content of the web page.\n",
        "      * Parses the HTML to extract the main text content.\n",
        "      * Wraps that extracted text into one or more Document objects.\n",
        "      * `Important Note`: The .load() method always returns a list of Document objects, even if it's just one document per webpage.\n",
        "* `Result (docs variable)`:\n",
        "The docs variable will end up being a list of lists of Document objects.\n",
        "For example, `docs[0]` would contain a list (likely with just one Document object) representing the content of the \"reward-hacking\" post. `docs[1]` would contain the Document for the `\"hallucination\"` post, and so on.\n",
        "A Document object in `LangChain` typically has two main parts:\n",
        "  * `page_content`: The extracted text content from the web page.\n",
        "  * `metadata`: A dictionary containing additional information about the document, such as its source (the original URL), and potentially other details like title.\n",
        "\n",
        "# Significance for RAG Agents (The \"Retrieval\" Part Begins!)\n",
        "This code snippet is the critical first step in populating your RAG agent's knowledge base:\n",
        "\n",
        " * `Data Ingestion`: You are effectively `\"ingesting\"` raw, unstructured data (web pages) into your RAG pipeline.\n",
        " * `Foundation for Retrieval`: These docs objects represent the raw information your agent will eventually `\"retrieve\"` from. Without loading this data, your RAG agent would have nothing to search through.\n",
        " * `Preparation for Processing`: These Document objects are now ready for the next stages of a RAG pipeline, which typically involve:\n",
        " * `Text Splitting`: Breaking these large documents into smaller, more manageable \"chunks.\"\n",
        " * `Embedding`: Converting these chunks into numerical representations (vectors).\n",
        " * `Vector Storage`: Storing these embedded chunks in a searchable database (a vector store).\n",
        "\n",
        "This initial `WebBaseLoader` step is the very start of turning raw external information into something an LLM can effectively leverage for augmented generation!"
      ],
      "metadata": {
        "id": "fGgU4Z2VyvVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",\n",
        "    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
        "    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\",\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41_we2ip0JMx",
        "outputId": "0cbe8a03-3e04-4207-b168-06e5bdfe52f9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a nutshell, this line of code retrieves the raw text content of the first blog post you loaded, cleans up any unnecessary whitespace, and then displays only the first 1000 characters of that cleaned text.\n",
        "\n",
        "It's a debugging or inspection step to quickly verify that your WebBaseLoader successfully extracted meaningful text from the web pages.\n",
        "\n",
        "1. `docs:`\n",
        " * This refers to the Python list you just created in the previous step: `docs = [WebBaseLoader(url).load() for url in urls]`.\n",
        " * Remember, docs is a list of lists of Document objects. Each inner list typically contains one Document object for each URL you loaded.\n",
        "2. `docs[0]`:\n",
        " * This uses **list indexing** to access the first element of the docs list.\n",
        " * In your case, `docs[0]` will be the list of Document objects loaded from the first URL: \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\".\n",
        " * Since `WebBaseLoader().load()` always returns a list, `docs[0]` itself is a list (e.g., `[Document_for_url1]`).\n",
        "3. `docs[0][0]:`\n",
        " * This uses another level of list indexing. It accesses the first element of the list that is `docs[0]`.\n",
        " * This specifically retrieves the very first Document object that was loaded from the first URL (`\"reward-hacking\"` post).\n",
        "4. `.page_content:`\n",
        " * This is an attribute access on the Document object `(docs[0][0])`.\n",
        " * As discussed, a LangChain Document object has a `page_content` attribute, which stores the main textual content extracted from the source (in this case, the text of the Lilian Weng blog post).\n",
        " * So, `docs[0][0].page_content` gives you the full, raw text content of the first blog post as a single long string.\n",
        "5. `.strip():`\n",
        "* This is a standard string method in Python.\n",
        "* `strip()` removes any leading or trailing whitespace (spaces, tabs, newlines) from the beginning and end of the string.\n",
        "* `Why it's used here`: Web-scraped content often has extra whitespace at the start or end due to HTML structure, and `strip()` cleans that up, making the content more presentable.\n",
        "\n",
        "6. `[:1000]:`\n",
        "\n",
        " * This is string slicing.\n",
        " * It takes the string result of the previous operations `(docs[0][0].page_content.strip())` and extracts a substring.\n",
        " * `[:1000]` means \"start from the beginning of the string (index 0) and go up to (but not including) index 1000.\"\n",
        " * `Why it's used here:` The full content of a blog post can be very long. This slicing limits the output to just the first 1000 characters, which is usually enough to get a quick glance at the content and confirm that the loading process worked correctly without printing an enormous amount of text to your console.\n"
      ],
      "metadata": {
        "id": "EmUT5wfK05Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0][0].page_content.strip()[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "GIfilxla0NPM",
        "outputId": "9af3ed86-4412-4ea5-9fb1-7d47f7749fa1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Reward Hacking in Reinforcement Learning | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Reward Hacking in Reinforcement Learning\\n    \\nDate: November 28, 2024  |  Estimated Reading Time: 37 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nBackground\\n\\nReward Function in RL\\n\\nSpurious Correlation\\n\\n\\nLet’s Define Reward Hacking\\n\\nList of Examples\\n\\nReward hacking examples in RL tasks\\n\\nReward hacking examples in LLM tasks\\n\\nReward hacking examples in real life\\n\\n\\nWhy does Reward Hacking Exist?\\n\\n\\nHacking RL Environment\\n\\nHacking RLHF of LLMs\\n\\nHacking the Training Process\\n\\nHacking the Evaluator\\n\\nIn-Context Reward Hacking\\n\\n\\nGeneralization of Hacking Skills\\n\\nPeek into Mitigations\\n\\nRL Algorithm Improvement\\n\\nDetecting Reward Hacking\\n\\nData Analysis of RLHF\\n\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nReward hacking occurs when a reinforcement learning (RL) agent exploits flaws or ambiguities in the reward function to ac\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `from langchain_text_splitters import RecursiveCharacterTextSplitter`\n",
        " * `langchain_text_splitters:` This is the library you just installed, specifically designed for breaking down large pieces of text.\n",
        " * `RecursiveCharacterTextSplitter:` This is a powerful and commonly used text splitter provided by `LangChain`.\n",
        " * `\"Recursive\"`: It tries to split text by different characters (like `\\n\\n`, then `\\n`, then ) in a hierarchical way. It first tries to split by large, semantically meaningful separators. If the chunks are still too big, it moves to smaller separators. This helps to keep related sentences and paragraphs together, preserving more context.\n",
        " * `\"Character\"`: It primarily works by counting characters, but as you'll see, we'll configure it to count by tokens.\n",
        "\n",
        "2. `docs_list = [item for sublist in docs for item in sublist]`\n",
        " * `What docs is`: Recall that docs was a `\"list of lists of Document objects.\"` For example, if you loaded 3 URLs, docs looked something like `[[Doc1_for_URL1], [Doc1_for_URL2], [Doc1_for_URL3]]`.\n",
        " * `List Comprehension for Flattening:` This is a Python idiom for `\"flattening\"` a list of lists into a single, flat list.\n",
        " * `for sublist in docs`: It iterates through each inner list (e.g., `[Doc1_for_URL1])`.\n",
        "    * `for item in sublist`: For each sublist, it then iterates through each item (which is a Document object) within that sublist.\n",
        "    * `item`: Each Document object found is added to the new `docs_list`.\n",
        "\n",
        " * `Result (docs_list)`: docs_list will now be a single, flat list of Document  objects:  `[Doc1_for_URL1, Doc1_for_URL2, Doc1_for_URL3]`.\n",
        "\n",
        "Why this is necessary: Most text splitters in `LangChain` expect a flat list of Document objects as input, not a nested list.\n",
        "\n",
        "3. `text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(...)`\n",
        " * `text_splitter`: This creates an instance of your chosen text splitter, configured with specific rules.\n",
        " * `.from_tiktoken_encoder()`: This is a very important factory method! Instead of simply counting characters, it tells the `RecursiveCharacterTextSplitter` to use OpenAI's tiktoken library for counting.\n",
        "\n",
        "    * Why `tiktoken`? Large Language Models (LLMs) like `GPT-4o` don't process text character by character; they process it in `\"tokens.\"` tiktoken is OpenAI's tokenizer, which accurately calculates how many tokens a piece of text will consume.\n",
        "    * `Significance:` By using `from_tiktoken_encoder`, you ensure that your `chunk_size` and `chunk_overlap` are measured in the same units that your LLM understands, making it much more reliable to ensure chunks fit within context windows and to estimate costs.\n",
        " * `chunk_size=100:`\n",
        "   * This is the **maximum desired size of each text chunk**, measured in tokens (because of from_tiktoken_encoder). So, no chunk will be larger than 100 tokens.\n",
        " * `Trade-offs`:\n",
        "    * `Too small`: Might break up important context, making it harder for the LLM to understand.\n",
        "    * `Too large`: Might exceed the LLM's context window, or make retrieval less precise (you want very relevant chunks, not just giant blobs of text). 100 tokens is quite small for many applications, often a starting point for experimentation.\n",
        " * `chunk_overlap=50:`\n",
        "    * This specifies the number of tokens that adjacent chunks will share.\n",
        "    * For example, if Chunk A is tokens 0-99, and Chunk B is tokens 50-149, then tokens 50-99 are present in both.\n",
        " * **Why overlap?** It helps to preserve context. If a crucial piece of information or a sentence spans the boundary between two chunks, overlap ensures that context isn't lost. Without overlap, a query might retrieve only one half of a key sentence, leading to incomplete answers.\n",
        "\n",
        " 4. `doc_splits = text_splitter.split_documents(docs_list)`\n",
        "  * `text_splitter.split_documents(docs_list):` This calls the core method on your configured text_splitter. It takes your flat list of original Document objects `(docs_list)` as input.\n",
        "  * It then applies the `RecursiveCharacterTextSplitter's` logic:\n",
        "  * It recursively tries to split the content of each Document in `docs_list`.\n",
        "  * It attempts to create chunks of up to 100 tokens, with 50 tokens of overlap between them.\n",
        "Crucially, each new small chunk also becomes a Document object, inheriting the metadata (like source URL) from its original parent document.\n",
        " * `Result (doc_splits)`: `doc_splits` will be a new, flat list of many smaller Document objects. Each of these `Document` objects represents one \"chunk\" of text from your original blog posts.\n",
        "\n",
        "# Significance for RAG Agents (The \"Chunking\" Stage!)\n",
        "\n",
        "This entire code block is the \"Chunking\" stage of your RAG pipeline, which is absolutely vital:\n",
        "\n",
        " * `Context Window Management:` LLMs have limited \"context windows.\" Large documents won't fit. Chunking breaks them down so that relevant pieces can fit when retrieved.\n",
        " * `Improved Relevance:` When you perform a similarity search (the next step in RAG), you want to find the most relevant specific pieces of information to a user's query. Smaller, focused chunks are much more likely to be highly relevant than entire large documents.\n",
        " * `Cost Efficiency:` Sending only small, relevant chunks to the LLM (instead of whole documents) significantly reduces the number of tokens you consume, leading to lower API costs.\n",
        " * `Foundation for Embedding:` These `doc_splits` are now in the perfect format to be converted into numerical embeddings and stored in a vector database, making them searchable for your RAG agent!"
      ],
      "metadata": {
        "id": "mKsDyd_iFUUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=100, chunk_overlap=50\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)"
      ],
      "metadata": {
        "id": "IEgkDdGW0Uqo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line of code gets the text content of the very first chunk that your `RecursiveCharacterTextSplitter` created, and then removes any extra whitespace from its beginning and end.\n",
        "\n",
        "It's commonly used as a quick sanity check to see what your individual chunks look like after the splitting process, ensuring that the `chunk_size` and `chunk_overlap` parameters are producing sensible results. You're effectively peering into one small piece of your prepared knowledge base.\n",
        "\n",
        "It's a quick way to inspect one of the processed chunks of text you just created for your RAG agent.\n",
        "\n",
        "1. `doc_splits:`\n",
        "\n",
        "This refers to the Python list you created in the previous step. It contains all the smaller Document chunks that resulted from splitting your original, larger web pages using the `RecursiveCharacterTextSplitter`.\n",
        "\n",
        "2. `doc_splits[0]`:\n",
        "\n",
        "This uses list indexing to access the first element in the `doc_splits` list.\n",
        "So, `doc_splits[0]` gives you the very first Document object that represents one of those smaller, processed text chunks.\n",
        "\n",
        "3. `.page_content`:\n",
        "\n",
        "This is an attribute access on that Document object `(doc_splits[0])`.\n",
        "Just like the original Document objects from the `WebBaseLoader`, these new smaller chunk Document objects also have a `page_content` attribute. This attribute holds the actual text of that specific chunk.\n",
        "\n",
        "4. `.strip():`\n",
        "This is a standard string method in Python.\n",
        " * When called on `page_content`, `strip()` removes any leading or trailing whitespace (like spaces, tabs, or newlines) from the beginning and end of the text chunk.\n",
        " * Why it's used here: Text splitting can sometimes leave minor extra whitespace around the edges of a chunk, and `strip()` cleans this up for better readability and consistency."
      ],
      "metadata": {
        "id": "WYYXwJL7OKB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_splits[0].page_content.strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "G6a4rN2u0YR0",
        "outputId": "ecf8500b-19a1-4c39-a9ad-9eba5ea87717"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Reward Hacking in Reinforcement Learning | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Significance for RAG Agents (The \"Embedding & Storage for Search\" Stage!)\n",
        "This entire block of code is crucial for the `\"Retrieval\"` component of RAG:\n",
        "\n",
        " * `Semantic Search`: You're setting up the ability to search your custom knowledge base not just by keywords, but by meaning. If a user asks \"Tell me about models that trick humans,\" your agent can find chunks related to `\"reward hacking\"` because their embeddings are semantically close.\n",
        " * `External Knowledge:` This is how you give your LLM access to information beyond its training data cutoff. The `retriever` becomes the mechanism for the LLM to \"look up\" information from your Lilian Weng blog posts.\n",
        " * `Core of RAG`: The retriever will be the component you pass into your RAG chain or agent, allowing it to dynamically fetch relevant context before generating an answer.\n",
        "\n",
        "1. `from langchain_core.vectorstores import InMemoryVectorStore`\n",
        " * `langchain_core.vectorstores`: This module in `LangChain` provides interfaces and implementations for various vector stores (also known as vector databases).\n",
        " * InMemoryVectorStore:\n",
        "     * `What it is:` This is a very simple vector store that stores data entirely in your computer's RAM (memory).\n",
        "     * `Advantages`:\n",
        "      * `Easy to set up`: No external database or server is needed. You just instantiate it.\n",
        "      * `Fast for small datasets`: Operations are very quick because data is in memory.\n",
        "    * `Limitations`:\n",
        "      * `Not persistent`: All your data will be lost as soon as your Python script or notebook session ends.\n",
        "      * `Not scalable`: Not suitable for large amounts of data (tens of thousands or millions of documents) or for production environments where persistence and concurrent access are required. For larger projects, you'd switch to external vector databases like Chroma, FAISS, Pinecone, etc.\n",
        " * `Role in RAG`: It's where your text chunks (and their numerical representations) will live, ready to be searched.\n",
        "\n",
        "2. `from langchain_openai import OpenAIEmbeddings`\n",
        "* `langchain_openai`: This is the `LangChain` integration for OpenAI's services.\n",
        "* `OpenAIEmbeddings`:\n",
        "     * `What it is:` This class is `LangChain's` interface to OpenAI's text embedding models. The most commonly used model for this purpose is `text-embedding-ada-002` (or newer versions like `text-embedding-3-small/large`).\n",
        "     * `What are Embeddings?:` This is a core concept in RAG! An embedding model takes a piece of text (like your chunks of blog post content) and converts it into a dense vector of numbers (a list of floats).\n",
        "     * `The Magic:` Texts that are semantically similar (mean similar things) will have embedding vectors that are \"close\" to each other in this multi-dimensional space. Texts that are semantically different will have vectors far apart.\n",
        " * `Role in RAG:` Embeddings are how you make your text chunks searchable by meaning. When a user asks a question, their question is also converted into an embedding, and then the vector store finds chunks whose embeddings are numerically closest to the query's embedding.\n",
        " * `Cost`: Using `OpenAIEmbeddings` incurs a cost based on the number of tokens processed (both for embedding your `doc_splits` and for embedding user queries later).\n",
        "\n",
        "3. `vectorstore = InMemoryVectorStore.from_documents(...)`\n",
        "  * `vectorstore:` This variable will hold your initialized and populated `InMemoryVectorStore`.\n",
        "\n",
        "  * `.from_documents(...)`: This is a static method provided by       `InMemoryVectorStore` (and most other vector stores in `LangChain`) that conveniently handles a two-step process:\n",
        "    1. `documents=doc_splits:` It takes your `doc_splits` (the list of small Document chunks you created earlier).\n",
        "    2. `embedding=OpenAIEmbeddings():` For each Document in `doc_splits:`\n",
        "       * It extracts the `page_content`.\n",
        "       * It sends this `page_content` to the `OpenAIEmbeddings` model to get its numerical vector representation.\n",
        "       * It then stores this vector along with the original `page_content` and its metadata within the `InMemoryVectorStore`.\n",
        "   \n",
        "   `What this line does`: This single line effectively ingests your entire knowledge base (the Lilian Weng blog posts, now chunked) into a searchable vector database. It's where your text gets transformed into a format that allows for semantic similarity search.\n",
        "\n",
        "4. `retriever = vectorstore.as_retriever()`\n",
        "\n",
        " * `retriever:` This variable will hold an instance of a `LangChain Retriever`.   \n",
        "   * `What is a Retriever?`: In `LangChain`, a `Retriever` is an interface that is responsible for fetching relevant `Documents` given a query. It abstracts away the underlying mechanism of how the documents are retrieved (whether it's from a vector store, a traditional database, an API, etc.).\n",
        "   * `vectorstore.as_retriever():` This method converts your vectorstore (which knows how to store and search embeddings) into a `Retriever` object.\n",
        "     * By default, when you call this retriever, it will:\n",
        "       1. Take a user query.\n",
        "       2. Embed that query using the same `OpenAIEmbeddings` model.\n",
        "       3. Perform a similarity search in the `vectorstore` to find the chunks whose embeddings are most similar to the query's embedding.\n",
        "       4. Return the actual `Document` objects (with their `page_content and metadata`) that correspond to these top-scoring chunks.\n",
        "\n"
      ],
      "metadata": {
        "id": "3MZOUSekUq8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using OpenAIEmbeddings incurs a cost based on the number of tokens processed (both for embedding your doc_splits and for embedding user queries later).\n",
        "# text-embedding-ada-002-v2 will be used.\n",
        "# Usage Check: before & after execution: https://platform.openai.com/usage/embeddings\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "vectorstore = InMemoryVectorStore.from_documents(\n",
        "    documents=doc_splits, embedding=OpenAIEmbeddings()\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "R8SEDvgL0dzW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Significance for your RAG Agentic AI (Giving the Agent a \"Reading Skill\")\n",
        "This single line of code is the bridge that connects your RAG setup to your Agentic AI framework:\n",
        "\n",
        "  * `Toolification`: You're taking a retrieval capability and turning it into a discrete \"tool\" or \"skill\" that your LLM can consciously choose to use.\n",
        "  * `Dynamic Information Access`: Your agent no longer just relies on its pre-trained knowledge. If a user asks something specific that's covered in those Lilian Weng blog posts (e.g., \"What are the latest research challenges in reward hacking?\"), the LLM will recognize:\n",
        "     * \"I need specific information.\"\n",
        "     * \"My internal knowledge might be limited or outdated.\"\n",
        "     * \"Ah, I have a retrieve_blog_posts tool that's designed for this!\"\n",
        "  * `Enhanced Reasoning`: The LLM will then invoke this tool with the relevant part of the user's query. The tool will retrieve the most relevant chunks from your vector store, and these chunks will be provided back to the LLM as `\"observations.\"` The LLM can then use this newly retrieved, accurate information to formulate a much better, grounded, and non-hallucinatory answer.\n",
        "\n",
        "This `retriever_tool` will soon be added to the list of tools (like `add, multiply, search`) that your `llm_with_tools` agent can choose from, making it a powerful RAG agent capable of looking up specific domain knowledge.\n",
        "\n",
        "This step is all about taking your powerful retriever (which can search your custom knowledge base) and turning it into a \"tool\" that your Agentic AI (the LLM) can decide to use when it needs to look up information.\n",
        "\n",
        "1. `from langchain.tools.retriever import create_retriever_tool`\n",
        "   * `langchain.tools.retriever`: This module within `LangChain` provides utilities specifically for creating tools from\n",
        "   `retrievers`.\n",
        "     * `create_retriever_tool`: This is a convenience function that does exactly\n",
        "     * `what its name suggests:` it takes a retriever object and wraps it into a format that can be used as a `\"tool\"` by a `LangChain` agent.\n",
        "2. `retriever_tool = create_retriever_tool(...)`\n",
        "This line calls the function to create your new tool and assigns it to the variable `retriever_tool`. Let's look at the arguments it takes:\n",
        "\n",
        "   * `retriever:`\n",
        "     * This is the first argument, and it refers to the retriever object you created in the previous step `(retriever = vectorstore.as_retriever())`.\n",
        "      * `What it provides`: This is the core functionality of the tool. When this `retriever_tool` is invoked by the LLM, it's this underlying retriever that will perform the actual similarity search on your `InMemoryVectorStore` to find relevant blog post chunks based on the input query.\n",
        "   * `\"retrieve_blog_posts\"`: This is the second argument, and it's the name of the tool.\n",
        "\n",
        "   * `Crucial for Agentic AI`: This is the literal string that the LLM (like GPT-4o) will generate in its response if it decides to use this tool. For example, it might output `tool_code(\"retrieve_blog_posts\", \"What is reward hacking?\")`.\n",
        "   * It should be a concise, descriptive, and memorable name that the LLM can easily refer to.\n",
        "\n",
        "   * `\"Search and return information about Lilian Weng blog posts.\"`:\n",
        "\n",
        "     * This is the third argument, and it's the description of the tool.\n",
        "     * `Absolutely critical for Agentic AI`: This is the most important part for the LLM's decision-making. When `LangChain` binds this tool to your `llm_with_tools`, this description (along with the name and the tool's expected input type, which `create_retriever_tool` infers) is what the LLM `\"reads.\"`\n",
        "     * The LLM uses this description to understand:\n",
        "       * `What this tool is for:` \"It searches for info about Lilian Weng's blog posts.\"\n",
        "       * `When to use it`: If the user's query is about \"hallucination in LLMs\" or \"diffusion models for video\" (topics covered in those blog posts), the LLM will identify that this tool is relevant.\n",
        "       * `How to use it:` It implicitly understands that the tool takes a query (the user's question) as input."
      ],
      "metadata": {
        "id": "EGu0su6azwO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "retriever_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"retrieve_blog_posts\",\n",
        "    \"Search and return information about Lilian Weng blog posts.\",\n",
        ")"
      ],
      "metadata": {
        "id": "dS4yyTqK0gCS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This line is a direct demonstration of the `\"Retrieval\"` step in RAG:\n",
        "\n",
        "`Prototyping/Testing`: It allows you to quickly see what kind of information your RAG agent would retrieve if it were asked about \"types of reward hacking.\" This is invaluable for debugging your RAG setup (e.g., checking if your chunking and embedding are working correctly).\n",
        "\n",
        "`Simulating Agent Behavior`: When your full RAG agent (built with `LangGraph`) runs, if the LLM decides it needs external information about `\"reward hacking,\"` it will generate a tool call to `retrieve_blog_posts` with a query. The `ToolNode` in your graph would then internally perform exactly this `retriever_tool.invoke(...)` call.\n",
        "`Providing Context`: The output of this invoke call (the list of relevant Documents) would then be passed back to the LLM (as a `ToolMessage` in the conversation history), giving the LLM the necessary context to answer the user's question accurately and based on your provided knowledge base.\n",
        "\n",
        "# What happens when this line of code executes:\n",
        "\n",
        "When you run `retriever_tool.invoke({\"query\": \"types of reward hacking\"})`, here's the sequence of events under the hood:\n",
        "\n",
        "1. The `retriever_tool` receives the input `{\"query\": \"types of reward hacking\"}`.\n",
        "2. It takes the value of the `\"query\"` key `(\"types of reward hacking\")` and passes it to the underlying retriever object (which you defined as `vectorstore.as_retriever()`).\n",
        "3. The retriever then:\n",
        "  * Takes the query `\"types of reward hacking\"`.\n",
        "  * Uses your configured `OpenAIEmbeddings()` model to convert this query string into a numerical vector (an embedding).\n",
        "  * Performs a similarity search within the `InMemoryVectorStore` you populated with your blog post chunks. It compares the query's embedding to the embeddings of all the stored `doc_splits`.\n",
        "  * It identifies the `k (a default number, usually 4 or 5)` most semantically similar chunks from your `doc_splits`.\n",
        "  * It retrieves the original Document objects for these `top-k` chunks.\n",
        "4. Finally, the `retriever_tool.invoke()` call returns a list of Document objects. Each `Document` in this list will contain the `page_content` (the actual text chunk) and `metadata` (like the original URL source) of the text segments that are most relevant to the query `\"types of reward hacking.\"`"
      ],
      "metadata": {
        "id": "17ZVuDrj45Bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_tool.invoke({\"query\": \"types of reward hacking\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "sQlCKtAO0ieF",
        "outputId": "71ba4de7-ddae-4fe9-906d-45d120c0c7c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Detecting Reward Hacking#\\n\\nIn-Context Reward Hacking#\\n\\n(Note: Some work defines reward tampering as a distinct category of misalignment behavior from reward hacking. But I consider reward hacking as a broader concept here.)\\nAt a high level, reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering.\\n\\nWhy does Reward Hacking Exist?#'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This `generate_query_or_respond` function is your RAG agent's `\"Smart Retrieval Decision\"` node:\n",
        "\n",
        "  * It enables the agent to intelligently decide when to use its retrieval capability. It won't always retrieve, which makes it more efficient.\n",
        "  * It allows the agent to handle a broader range of queries:\n",
        "    * Queries needing external knowledge -> use retriever_tool.\n",
        "    * General conversational queries -> respond directly.\n",
        "  * It forms a critical part of the control flow for your RAG agent in LangGraph, determining whether the execution should move to a tool execution node or directly to a final answer node.\n",
        "\n",
        "  1. Imports and LLM Initialization:\n",
        "\n",
        " * `from langgraph.graph import MessagesState:`\n",
        "      * This imports `MessagesState`, a pre-built `TypedDict` from `LangGraph`. It's a convenient way to define a graph state that primarily consists of a list of messages and automatically handles appending new messages.\n",
        "      * Note: In your previous custom `GraphState`, you explicitly defined messages: `Annotated[list[AnyMessage], operator.add]`. `MessagesState` essentially provides this messages field with the `operator.add` behavior out of the box. Here, it's used as a type hint for the state argument.\n",
        "\n",
        " * `from langchain.chat_models import init_chat_model`:\n",
        "      * `Important Note`: This `init_chat_model` function is an older (and now deprecated) way to initialize chat models in `LangChain`.\n",
        "      * `Recommendation`: For modern `LangChain` code, you should consistently use from `langchain_openai` import `ChatOpenAI` as you did before. It's cleaner and the current standard.\n",
        " * `response_model = init_chat_model(\"openai:gpt-4.1\", temperature=0):`\n",
        "      * This initializes an instance of an OpenAI chat model.\n",
        "      * \"openai:gpt-4.1\": This model name is unusual and likely an internal alias or placeholder from an older example. For current use, you should use publicly available model names like `\"gpt-4o\", \"gpt-4-turbo\", \"gpt-3.5-turbo\"`, etc. For your RAG agent, you'll likely want to use `gpt-4o`.\n",
        "      * `temperature=0`: This sets the model's \"creativity\" to its lowest possible value. A temperature of 0 (or close to it) makes the model's responses highly deterministic and factual. This is ideal for RAG, as you want the model to be precise in its decisions (retrieve or respond) and factual in its generation (based on retrieved content).\n",
        "2. `def generate_query_or_respond(state: MessagesState):`\n",
        "This defines a Python function that will serve as a node in your LangGraph workflow.\n",
        "   * `state: MessagesState`: This is the input to the node. `LangGraph` will pass the current state of the graph to this function. The `MessagesState` type hint tells us that this state dictionary is expected to contain a messages key, which is a list of `AnyMessage` objects (your conversation history).\n",
        "   * `Docstring`: The docstring clearly explains the purpose of this node: it will use the LLM to decide if it needs to use the `retriever_tool` (i.e., search the knowledge base) or if it can simply answer the user's question directly.\n",
        "\n",
        "3. `response = (response_model.bind_tools([retriever_tool]).invoke(state[\"messages\"]))`\n",
        "   * This is the core logic of this node and the key to its decision-making ability:\n",
        "   * `response_model`: This refers to the initialized LLM (`gpt-4.1` or preferably `gpt-4o`).\n",
        "   * `.bind_tools([retriever_tool])`: This is a critical step!\n",
        "\n",
        "     * You are temporarily \"binding\" (or teaching) only the `retriever_tool` to this specific LLM instance (response_model) for this particular invocation.\n",
        "     * This means when the LLM processes the input, it is aware that it has one specific tool available: `\"retrieve_blog_posts\"` with its description (\"Search and return information about Lilian Weng blog posts.\").\n",
        "  * `.invoke(state[\"messages\"])`: The entire conversation history (state[\"messages\"]) is passed to the LLM as the context.\n",
        "  * `What the LLM does (Decision-Making):` Based on the messages it receives and the `retriever_tool` it knows about, the LLM will make a decision:\n",
        "\n",
        "     * `Option 1: Call the Tool (Retrieve)`: If the LLM determines that the user's latest message (or the overall conversation) requires information that is likely present in the Lilian Weng blog posts (e.g., \"What is 'reward hacking'?\"), it will output an `AIMessage` that contains a tool_calls attribute indicating it wants to invoke `retrieve_blog_posts` with the relevant query (e.g., query=\"reward hacking\").\n",
        "     * `Option 2:` Direct Response (Don't Retrieve): If the LLM determines it can answer the question directly from its own general knowledge or if the query is a simple greeting (e.g., \"Hello,\" \"How are you?\"), it will output a plain `AIMessage` with its textual response and no `tool_calls` attribute.\n",
        "  * `response`: This variable will store the `AIMessage` object that the LLM returns, containing either a tool call or a direct answer.\n",
        "\n",
        "4. `return {\"messages\": [response]}`\n",
        "  * This line returns a dictionary to `LangGraph`, telling it how to update the graph's global state.\n",
        "  * `\"messages\": [response]`: The `AIMessage` (whether it's a tool call or a direct answer) returned by the LLM is wrapped in a list.\n",
        "  * `How LangGraph handles it:` Since your graph's state is configured as `MessagesState` (or similar with operator.add for messages), `LangGraph` will automatically append this `[response]` to the existing messages list in the global state. This updates the conversation history with the LLM's latest decision/response."
      ],
      "metadata": {
        "id": "mRO_ANnvDHyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import MessagesState\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "response_model = init_chat_model(\"openai:gpt-4o\", temperature=0)\n",
        "\n",
        "\n",
        "def generate_query_or_respond(state: MessagesState):\n",
        "    \"\"\"Call the model to generate a response based on the current state. Given\n",
        "    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.\n",
        "    \"\"\"\n",
        "    response = (\n",
        "        response_model\n",
        "        .bind_tools([retriever_tool]).invoke(state[\"messages\"])\n",
        "    )\n",
        "    return {\"messages\": [response]}"
      ],
      "metadata": {
        "id": "oL3UruL80wW_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What this line accomplishes:\n",
        "\n",
        "This single line of code effectively simulates a user saying \"hello!\" to your RAG agent's core decision-making brain (`generate_query_or_respond`) and then immediately prints out the agent's direct textual response.\n",
        "\n",
        "You'll observe that for a simple greeting, the LLM will not invoke the `retriever_tool`. This demonstrates that your agent is smart enough to handle straightforward conversational turns without needlessly performing a retrieval step, which is good for efficiency and user experience."
      ],
      "metadata": {
        "id": "rceuKDSUMimy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = {\"messages\": [{\"role\": \"user\", \"content\": \"hello!\"}]}\n",
        "generate_query_or_respond(input)[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voaIxUvs0zAt",
        "outputId": "44486aa6-e226-4580-de4a-4ca9a5905531"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line of code simulates a user asking a question that requires external knowledge and then immediately shows you the agent's internal decision to perform a retrieval action.\n",
        "\n",
        "This is the \"Reason\" part of the ReAct loop in action for your RAG agent. The agent has reasoned that it needs to Act (by retrieving information) before it can fully Respond. This test"
      ],
      "metadata": {
        "id": "cNZMCzzZNpcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = {\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "generate_query_or_respond(input)[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzE2mK3h08Nt",
        "outputId": "0dfd0c04-e0d3-4c11-f502-628aef20fe90"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  retrieve_blog_posts (call_20cIJC07fA3D9xSdAxeXkM1L)\n",
            " Call ID: call_20cIJC07fA3D9xSdAxeXkM1L\n",
            "  Args:\n",
            "    query: types of reward hacking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Significance for your RAG Agentic AI:\n",
        "This `grade_documents` node introduces a **powerful self-correction** mechanism into your RAG agent:\n",
        "\n",
        "  * `Relevance Check`: The agent doesn't just blindly use whatever it retrieves. It critiques its own retrieval results.\n",
        "  * `Reduces Hallucinations`: By checking **relevance**, it prevents the LLM from trying to answer questions using irrelevant or misleading context, which significantly reduces the risk of generating inaccurate or hallucinated responses.\n",
        "  * `Robustness`: It makes your RAG agent more robust. If the initial retrieval fails to find relevant information, the agent can intelligently decide to retry (e.g., by rewriting the query) instead of giving a poor answer or failing outright.\n",
        "  * `Dynamic Flow`: It's a prime example of how `LangGraph` allows for dynamic, conditional execution paths based on the results of intermediate steps (like an LLM's decision).\n",
        "\n",
        "This is a sophisticated and crucial part of building an `advanced RAG (Retrieval-Augmented Generation)` agent, specifically enabling **self-correction and intelligent decision-making** about retrieval quality. This code defines a \"grader\" node for your `LangGraph` that determines if the retrieved documents are actually relevant to the user's question.\n",
        "\n",
        "1. `Imports for Structured Output and Type Hinting:`\n",
        "  * `from pydantic import BaseModel, Field:`\n",
        "      * `Pydantic`: A Python library used for data validation and parsing using Python type hints.\n",
        "      * `BaseModel`: The base class for creating data models (like schemas for dictionaries).\n",
        "      * `Field`: Used within Pydantic models to add more metadata (like descriptions) to fields.\n",
        "      * Why here?: This is crucial for getting structured, reliable output from an LLM. Instead of the LLM just freestyling text, we instruct it to output a specific JSON format that can be automatically parsed into a `GradeDocuments` object. This makes it easy to reliably extract the \"yes\" or \"no\" score.\n",
        "  * `from typing import Literal`:\n",
        "     * `Literal`: A type hint that restricts a variable's value to a specific set of literal values `(e.g., Literal[\"yes\", \"no\"])`.\n",
        "     * `Why here?`: It's used in the function `signature -> Literal[\"generate_answer\", \"rewrite_question\"]` to explicitly tell `LangGraph` (and anyone reading the code) that this node will return one of these two specific string values, which correspond to edges in your graph.\n",
        "\n",
        "2.` GRADE_PROMPT (The Grader's Instructions):`\n",
        "  * This is a multi-line string containing instructions for an LLM.\n",
        "  * `Purpose`: It defines the \"persona\" and task for a specialized LLM call. This LLM's job is to act as a \"grader\" and assess the relevance of retrieved information.\n",
        "  * `Inputs`: It uses `f-string-like` formatting `({context}, {question})` as placeholders for the actual retrieved content and the user's original question.\n",
        "  * `Core Instruction`: \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\" This guides the LLM to produce a very specific, actionable output.\n",
        "\n",
        "3. `class GradeDocuments(BaseModel):` (The Grader's Output Schema):\n",
        "   * This defines a Pydantic model named `GradeDocuments`.\n",
        "   * `binary_score: str = Field(...)`: This specifies that any object conforming to `GradeDocuments` must have a field named `binary_score`, which is a string. The Field provides a description that will be used by the LLM's function-calling capability.\n",
        "   * `Significance`: This tells the LLM (when with_structured_output is used) to output its response in a structured JSON format like `{\"binary_score\": \"yes\"} or {\"binary_score\": \"no\"}`. This is incredibly reliable for programmatic parsing compared to trying to extract \"yes\" or \"no\" from free-form text.\n",
        "  \n",
        "4. `grader_model = init_chat_model(...)` (The Grader LLM Instance):\n",
        "   * This initializes another instance of an OpenAI chat model.\n",
        "   * `\"openai:gpt-4.1\"`: Again, this is likely a placeholder. For actual use, choose a public model like gpt-4o or gpt-4-turbo.\n",
        "   * `temperature=0`: Crucial here! You want the grader LLM to be as deterministic and consistent as possible in its \"yes\"/\"no\" judgment. A low temperature minimizes creativity and randomness.\n",
        "\n",
        "5. `def grade_documents(...)` **(The LangGraph Node Function):**\n",
        "   * This function (`grade_documents`) will be defined as a node in your `LangGraph` workflow.\n",
        "    * `state: MessagesState`: It receives the current state of the graph, which contains the ongoing messages (conversation history).\n",
        "    * -> `Literal[\"generate_answer\", \"rewrite_question\"]`: This is its return type. Unlike nodes that return state updates `(like {\"messages\": [...]})`, this node returns a string that explicitly tells `LangGraph` which edge to follow next.\n",
        "\n",
        "      * \"generate_answer\": If the documents are deemed relevant.\n",
        "      * \"rewrite_question\": If the documents are deemed not relevant.\n",
        "\n",
        "\n",
        "# Inside grade_documents:\n",
        "  * `question = state[\"messages\"][0].content`: This line assumes that the original user question is the very first message in the `state[\"messages\"]` list.\n",
        "  * `context = state[\"messages\"][-1].content`: This line assumes that the retrieved documents are the last message added to the `state[\"messages\"]` list. This usually happens when a `ToolNode` (like the one that executes your retriever_tool) puts the tool's output into the messages list as a `ToolMessage` or similar, and its content is the stringified retrieved documents.\n",
        "  * `prompt = GRADE_PROMPT.format(...):` The GRADE_PROMPT template is filled with the actual question and context from the current graph state.\n",
        "  * `response = (grader_model.with_structured_output(GradeDocuments).invoke([...])):`\n",
        "    * This is the core call to the grader LLM.\n",
        "    * `.with_structured_output(GradeDocuments):`  It forces the `grader_model` to respond in the exact format defined by your `GradeDocuments` Pydantic model. The LLM's response will be automatically parsed into an instance of `GradeDocuments`.\n",
        "    * `invoke([{\"role\": \"user\", \"content\": prompt}])`: The formatted prompt is sent to the LLM.\n",
        "\n",
        "  * `score = response.binary_score`: The `binary_score` (either \"yes\" or \"no\") is directly extracted from the parsed Pydantic response object.\n",
        "\n",
        " * `Conditional Logic`:\n",
        "    * `if score == \"yes\": return \"generate_answer\":` If the grader LLM says \"yes,\" the graph flow is directed to a node that will generate the final answer to the user.\n",
        "    * `else: return \"rewrite_question\":` If the grader LLM says \"no,\" the graph flow is directed to a node that will handle the scenario where the retrieval was unsuccessful, typically by attempting to reformulate the question or trying a different retrieval strategy.\n"
      ],
      "metadata": {
        "id": "DwyrCSFjSp1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal\n",
        "\n",
        "GRADE_PROMPT = (\n",
        "    \"You are a grader assessing relevance of a retrieved document to a user question. \\n \"\n",
        "    \"Here is the retrieved document: \\n\\n {context} \\n\\n\"\n",
        "    \"Here is the user question: {question} \\n\"\n",
        "    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"\n",
        "    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n",
        ")\n",
        "\n",
        "\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Grade documents using a binary score for relevance check.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"\n",
        "    )\n",
        "\n",
        "\n",
        "grader_model = init_chat_model(\"openai:gpt-4.1\", temperature=0)\n",
        "\n",
        "\n",
        "def grade_documents(\n",
        "    state: MessagesState,\n",
        ") -> Literal[\"generate_answer\", \"rewrite_question\"]:\n",
        "    \"\"\"Determine whether the retrieved documents are relevant to the question.\"\"\"\n",
        "    question = state[\"messages\"][0].content\n",
        "    context = state[\"messages\"][-1].content\n",
        "\n",
        "    prompt = GRADE_PROMPT.format(question=question, context=context)\n",
        "    response = (\n",
        "        grader_model\n",
        "        .with_structured_output(GradeDocuments).invoke(\n",
        "            [{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "    )\n",
        "    score = response.binary_score\n",
        "\n",
        "    if score == \"yes\":\n",
        "        return \"generate_answer\"\n",
        "    else:\n",
        "        return \"rewrite_question\""
      ],
      "metadata": {
        "id": "oGQCiyEP1BEB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This test case confirms that your `grade_documents` function correctly identifies irrelevant content (represented by `\"meow\"`) and, based on that, returns the `rewrite_question` signal.\n",
        "\n",
        "This is critical because:\n",
        "\n",
        "* It demonstrates the self-correction mechanism in action.\n",
        "* It ensures that if your retrieval system ever returns `garbage` or `irrelevant data`, your agent won't proceed to generate an answer based on it. Instead, it will get a signal to try a different approach (like rewriting the query or trying another retrieval method), making your RAG agent much more robust and less prone to giving bad answers from poor retrieval.\n",
        "\n",
        "You're now running a direct test of your `grade_documents` function, specifically focusing on its ability to identify `irrelevant retrieved content`. This code snippet simulates a scenario where the `retriever_tool` has been called, but the `\"retrieved\"` information is unhelpful.\n",
        "\n",
        "1. `from langchain_core.messages import convert_to_messages:`\n",
        "\n",
        "  * This imports a utility function from `LangChain` that helps convert raw Python dictionaries (which define messages like `\"role\"`, `\"content\"`, etc.) into proper `LangChain BaseMessage` objects (`HumanMessage, AIMessage, ToolMessage`).\n",
        "  * `Why it's used`: While `LangGraph` can often handle dictionary representations of messages, converting them explicitly ensures that all messages in your state are consistently typed `LangChain` objects, which is robust and often necessary for more complex `LangChain/LangGraph` operations.\n",
        "2. `input = {...}:`\n",
        "\n",
        "  * This dictionary represents the state that will be passed to your `grade_documents` function. It's designed to simulate a specific point in your agent's conversation history.\n",
        "\n",
        "  * `\"messages\": convert_to_messages([...])`: This is the core of the simulated history. Let's look at the messages within the list:\n",
        "\n",
        "  * **First Message (User's Original Question):** This simulates the initial `HumanMessage` from the user. When `grade_documents` runs, `question = state[\"messages\"][0].content` will extract this text.\n",
        "  * **Second Message (Assistant's Tool Call):**  This simulates the `AIMessage` that your `generate_query_or_respond` function would have returned if it decided to use the `retriever_tool`. It has empty content but crucially contains a `tool_calls` attribute.\n",
        "  * **Third Message (Tool's Output - SIMULATED IRRELEVANCE):**\n",
        "      * This simulates a `ToolMessage`, which is what gets added to the messages list after a tool (like `retrieve_blog_posts`) has been executed.\n",
        "      * `\"content\": \"meow\":` This is the key part of this test. You are deliberately providing `\"meow\"` as the retrieved `\"context\"` instead of actual relevant blog post content. This is to explicitly test the `\"irrelevant\"` path of your `grade_documents` function.\n",
        "      * `\"tool_call_id\": \"1\"`: Links this tool output back to the tool call that generated it.\n",
        "  * **grade_documents(input):** This directly calls your `grade_documents` function, passing it the input dictionary which contains this simulated conversation history."
      ],
      "metadata": {
        "id": "PdtA3Falykew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import convert_to_messages\n",
        "\n",
        "input = {\n",
        "    \"messages\": convert_to_messages(\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"\", # Empty content means the assistant didn't directly respond\n",
        "                \"tool_calls\": [  # This signals a tool call\n",
        "                    {\n",
        "                        \"id\": \"1\",\n",
        "                        \"name\": \"retrieve_blog_posts\", # The tool it decided to call\n",
        "                        \"args\": {\"query\": \"types of reward hacking\"}, # Arguments for the tool\n",
        "                    }\n",
        "                ],\n",
        "            },\n",
        "            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n",
        "        ]\n",
        "    )\n",
        "}\n",
        "grade_documents(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uuzSbdME1FHM",
        "outputId": "8a0b315c-a3f8-40b9-862e-f6c7a0dd3efd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'rewrite_question'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This test case confirms that your `grade_documents` function correctly identifies relevant content and, based on that, provides the appropriate signal `(\"generate_answer\")`.\n",
        "\n",
        "This is essential for the success of your RAG agent because it ensures that:\n",
        "\n",
        "  * When your retrieval system successfully finds good information, the agent's workflow proceeds to the next logical step: using that information to generate an answer for the user.\n",
        "  * It validates the positive path of your **self-correction mechanism**, ensuring the agent doesn't get stuck in a loop of rewriting questions when it already has good data.\n",
        "\n",
        "Together, these two test cases (`\"meow\"` and the relevant text) provide confidence that your `grade_documents` node can intelligently direct the flow of your RAG agent based on the quality of its retrieval.\n",
        "\n",
        "You're running another crucial test for your `grade_documents` function! This time, you're simulating a scenario where the `retriever_tool` has been called, and it successfully returns relevant information. This is designed to test the `\"relevant\"` path of your grading logic.\n",
        "\n",
        "1. `input = {...}`\n",
        "\n",
        "  * As before, this dictionary represents the state passed to `grade_documents`, simulating a specific point in the conversation.\n",
        "  * Messages within input:\n",
        "    * **`User Message:`** Still `\"What does Lilian Weng say about types of reward hacking?\"`. This is the question that the `relevance` will be judged against.\n",
        "    * **`Assistant Message (Tool Call):`** Still simulates the LLM's decision to call `retrieve_blog_posts` with the query `\"types of reward hacking.\"`\n",
        "  * **Tool Message (Tool Output - NOW RELEVANT!):**\n",
        "    * `\"content\"`: This is the critical change. Instead of `\"meow\"`, the content is now a snippet of text that is directly relevant to the user's question (\"types of reward hacking\"). This simulates a successful retrieval from your `InMemoryVectorStore`.\n",
        "2. `grade_documents(input):`\n",
        "  * This directly calls your `grade_documents` function, passing it this new input dictionary with the relevant simulated history."
      ],
      "metadata": {
        "id": "V6npbHUG5VFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = {\n",
        "    \"messages\": convert_to_messages(\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"\",\n",
        "                \"tool_calls\": [\n",
        "                    {\n",
        "                        \"id\": \"1\",\n",
        "                        \"name\": \"retrieve_blog_posts\",\n",
        "                        \"args\": {\"query\": \"types of reward hacking\"},\n",
        "                    }\n",
        "                ],\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"tool\",\n",
        "                \"content\": \"reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering\",\n",
        "                \"tool_call_id\": \"1\",\n",
        "            },\n",
        "        ]\n",
        "    )\n",
        "}\n",
        "grade_documents(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TAfK3r3m1Kb5",
        "outputId": "32dc79fd-20c6-4118-d2fb-f0e11461051d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'generate_answer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'generate_answer'"
      ],
      "metadata": {
        "id": "pT1yDwnJ1NbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This rewrite_question node completes a vital `self-correction` loop in your RAG agent:\n",
        "\n",
        "  1. **Iterative Improvement**: If the `grade_documents` node determines that the retrieved context was irrelevant, the graph flows to this `rewrite_question` node. Instead of giving up, the agent now intelligently rephrases its initial query.\n",
        "  2. **Enhanced Retrieval Success:** A more precise or semantically aligned question has a higher chance of retrieving relevant documents in the subsequent retrieval attempt.\n",
        "  3. **User Experience:** It creates a more robust and helpful agent. The user doesn't necessarily see this internal rewriting process; they just experience a more effective system that eventually finds the right answer.\n",
        "  4. **Dynamic Looping:** In your `LangGraph`, after this node returns, the flow would typically go back to the `generate_query_or_respond` node (or directly to the retriever_tool execution node) with the newly added, rewritten question in the messages history. This triggers another retrieval attempt.\n",
        "\n",
        "You're continuing to build out the sophisticated self-correction capabilities of your RAG agent! This code snippet defines a node that's responsible for rewriting the user's original question if the initial retrieval attempt failed to find relevant information. This is part of the agent's strategy to try again.\n",
        "\n",
        "1. `REWRITE_PROMPT (The Rewriter's Instructions)`:\n",
        "  * This is a string template containing instructions for an LLM.\n",
        "  * **`Purpose`:** It directs an LLM to act as a `\"question rewriter.\"` Its goal is to take an initial question and rephrase it in a way that might lead to better search results in a subsequent retrieval attempt.\n",
        "  * **`Instruction:`** `\"Look at the input and try to reason about the underlying semantic intent / meaning.\"` This encourages the LLM to go beyond superficial changes and genuinely improve the query.\n",
        "  * `Input`: It takes a single placeholder: `{question}` for the user's original query.\n",
        "2. `def rewrite_question(state: MessagesState): (The LangGraph Node Function):`\n",
        "  * This function defines another node in your `LangGraph` workflow.\n",
        "  * `state: MessagesState:` It receives the current state of the graph, which, as you know, contains the ongoing messages `(conversation history)`.\n",
        "  * `Return Type (Implicit):` This function will return a dictionary that updates the graph's state, specifically by adding a new `HumanMessage` to the messages list."
      ],
      "metadata": {
        "id": "yD-3eYzCGD3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "REWRITE_PROMPT = (\n",
        "    \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\"\n",
        "    \"Here is the initial question:\"\n",
        "    \"\\n ------- \\n\"\n",
        "    \"{question}\"\n",
        "    \"\\n ------- \\n\"\n",
        "    \"Formulate an improved question:\"\n",
        ")\n",
        "\n",
        "\n",
        "def rewrite_question(state: MessagesState):\n",
        "    \"\"\"Rewrite the original user question.\"\"\"\n",
        "    messages = state[\"messages\"] # Access the conversation history\n",
        "    question = messages[0].content # Get the original user's question\n",
        "    prompt = REWRITE_PROMPT.format(question=question) # Format the prompt for the LLM\n",
        "    # Use the main response_model to generate the rewritten question\n",
        "    # Note: No tools are bound here, as this LLM call is purely for generation (text rewriting)\n",
        "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
        "    # Return a new user message with the rewritten content to the state\n",
        "    return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]}"
      ],
      "metadata": {
        "id": "dc_IIcWB1QXO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code directly demonstrates your RAG agent's ability to **intelligently self-correct** its query. It confirms that when the agent's internal grading mechanism identifies irrelevant retrieved content, it doesn't give up. Instead, it leverages the LLM to reformulate the original question in an attempt to get better results on a subsequent retrieval try.\n",
        "\n",
        "The output you'll see will be the LLM's improved version of `\"What does Lilian Weng say about types of reward hacking?\"`, which will then be used in the next step of your `LangGraph` to perform another search. This is a critical component for building a robust and resilient RAG system.\n",
        "\n",
        "1. `input = {...}:`\n",
        "\n",
        "  * This is the same input dictionary you used to test the `\"irrelevant\"` path of `grade_documents`.\n",
        "  * `Significance:` This state is precisely what your `LangGraph` would pass to the `rewrite_question` node if the `grade_documents` node had just returned `\"rewrite_question\"`.\n",
        "2. `print(response[\"messages\"][-1][\"content\"]):`\n",
        "\n",
        "  * `response` is the dictionary returned by `rewrite_question`.\n",
        "  * `response[\"messages\"]` accesses the list of messages from that returned dictionary. This list will contain only one message: the newly generated `HumanMessage` with the rewritten question.\n",
        "  * `[-1]` gets that single (last) message.\n",
        "  * `[\"content\"]` extracts the actual text string of the rewritten question.\n",
        "  * Finally, `print()` displays this rewritten question to your console."
      ],
      "metadata": {
        "id": "5KmV_sjJLqOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = {\n",
        "    \"messages\": convert_to_messages(\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"\",\n",
        "                \"tool_calls\": [\n",
        "                    {\n",
        "                        \"id\": \"1\",\n",
        "                        \"name\": \"retrieve_blog_posts\",\n",
        "                        \"args\": {\"query\": \"types of reward hacking\"},\n",
        "                    }\n",
        "                ],\n",
        "            },\n",
        "            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n",
        "        ]\n",
        "    )\n",
        "}\n",
        "\n",
        "response = rewrite_question(input)\n",
        "print(response[\"messages\"][-1][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwYpqi_J1TEC",
        "outputId": "afa2e977-c091-491a-b235-4c123eccdb3f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What are the different types of reward hacking discussed by Lilian Weng?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GENERATE_PROMPT = (\n",
        "    \"You are an assistant for question-answering tasks. \"\n",
        "    \"Use the following pieces of retrieved context to answer the question. \"\n",
        "    \"If you don't know the answer, just say that you don't know. \"\n",
        "    \"Use three sentences maximum and keep the answer concise.\\n\"\n",
        "    \"Question: {question} \\n\"\n",
        "    \"Context: {context}\"\n",
        ")\n",
        "\n",
        "\n",
        "def generate_answer(state: MessagesState):\n",
        "    \"\"\"Generate an answer.\"\"\"\n",
        "    question = state[\"messages\"][0].content\n",
        "    context = state[\"messages\"][-1].content\n",
        "    prompt = GENERATE_PROMPT.format(question=question, context=context)\n",
        "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
        "    return {\"messages\": [response]}"
      ],
      "metadata": {
        "id": "3j7tjlxp1ao1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This last piece of the puzzle defines the answer generation node. This is where your agent finally uses all the hard work of retrieval and grading to provide a concise, informed answer to the user.\n",
        "\n",
        "This `generate_answer` node is the final and most visible part of your RAG agent's operation. It's where all the previous steps `(loading, chunking, embedding, retrieving, and grading)` culminate:\n",
        "\n",
        "  * `Grounded Answers`: This is where your RAG agent truly shines, providing answers grounded in your specific knowledge base `(the Lilian Weng blog posts)` rather than just the LLM's general training data.\n",
        "  * `Mitigating Hallucinations`: By explicitly instructing the LLM to use only the provided context and to admit if it `\"doesn't know,\"` you significantly reduce the risk of the LLM fabricating information.\n",
        "  * `Concise Output`: The prompt constraints help ensure a focused and user-friendly response.\n",
        "You now have all the individual building blocks (nodes) for a complete, self-correcting RAG agent in `LangGraph!`\n",
        "\n",
        "1. `GENERATE_PROMPT (The Answerer's Instructions)`\n",
        "  * This is a string template containing the system prompt that will guide the LLM's answer generation.\n",
        "  * `Purpose:` It clearly defines the LLM's role as a `\"question-answering assistant\"` and sets the rules for how it should generate a response.\n",
        "  * `Inputs:` It takes two placeholders:\n",
        "      * `{question}:` The original user's question.\n",
        "      *  `{context}:` The relevant information retrieved from your knowledge base.\n",
        "\n",
        "2. `def generate_answer(state: MessagesState):` `(The LangGraph Node Function)`\n",
        "  * This function defines another node in your `LangGraph` workflow. This node is typically executed when the `grade_documents` node has confirmed that the retrieved context is indeed relevant.\n",
        "  * `state: MessagesState:` It receives the current state of the graph, which includes the conversation history.\n",
        "  * `Return Type (Implicit):` This function will return a dictionary to update the graph's state by adding the generated answer as a new `AIMessage`."
      ],
      "metadata": {
        "id": "aFS89hajQFPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = {\n",
        "    \"messages\": convert_to_messages(\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"\",\n",
        "                \"tool_calls\": [\n",
        "                    {\n",
        "                        \"id\": \"1\",\n",
        "                        \"name\": \"retrieve_blog_posts\",\n",
        "                        \"args\": {\"query\": \"types of reward hacking\"},\n",
        "                    }\n",
        "                ],\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"tool\",\n",
        "                \"content\": \"reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering\",\n",
        "                \"tool_call_id\": \"1\",\n",
        "            },\n",
        "        ]\n",
        "    )\n",
        "}\n",
        "\n",
        "response = generate_answer(input)\n",
        "response[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PieL5XPI1dl8",
        "outputId": "1b993a1a-e5f8-4ca9-c17d-c70723170256"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Lilian Weng categorizes reward hacking into two types: environment or goal misspecification, and reward tampering.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        " 1. `from langgraph.graph import StateGraph, START, END`\n",
        "`from langgraph.prebuilt import ToolNode`\n",
        "`from langgraph.prebuilt import tools_condition`\n",
        "\n",
        "   * `StateGraph`: This is the main class from `langgraph` that you use to define your workflow. It's like the blueprint for your robot's brain.\n",
        "   * `START, END`: These are special, pre-defined nodes that represent the beginning and end of your agent's process.\n",
        "   * `ToolNode`: This is a powerful, pre-built `LangGraph` node. Its job is simple: if an LLM outputs a `\"tool call\"` (like \"I need to use `retrieve_blog_posts` with this query\"), `ToolNode` automatically executes that tool and puts the tool's output back into the graph's state (specifically, into the messages list as a `ToolMessage`).\n",
        "   * `tools_condition`: This is a helper function that helps `add_conditional_edges` make decisions. It looks at the LLM's output message. If the LLM has decided to call a tool, `tools_condition` returns a specific string (usually \"tools\"). If the LLM has decided to respond directly (no tool calls), it returns END.\n",
        "\n",
        " 2. `workflow = StateGraph(MessagesState)`\n",
        "   * You're creating an instance of `StateGraph` and calling it workflow.\n",
        "   * `MessagesState`: This tells `LangGraph` that the shared memory for this workflow will be managed as a list of conversational messages. Any node that returns `{\"messages\": [new_message]}` will automatically append `new_message` to the history.\n",
        "\n",
        "    3. `Defining Nodes (workflow.add_node(...))`\n",
        "This is where you tell the graph about all the individual `\"skills\"` or `\"steps\"` your robot can perform:\n",
        "    * `workflow.add_node(generate_query_or_respond):` Adds your function that uses the LLM to decide whether to search (`retrieve_blog_posts`) or just answer directly. This is your robot's `\"Decision Maker\"`.\n",
        "    * `workflow.add_node(\"retrieve\", ToolNode([retriever_tool]))`\n",
        "      * you're adding a node named `\"retrieve\"`.\n",
        "      * Its functionality is handled by a `ToolNode`.\n",
        "      * `ToolNode([retriever_tool]):` This `ToolNode` is configured to execute your `retriever_tool` (the one you made from `vectorstore.as_retriever())`. This is your robot's `\"Information Searcher\"` skill.\n",
        "    * `workflow.add_node(rewrite_question):` Adds your function that rewrites the user's question. This is your robot's `\"Question Refiner\"` skill.\n",
        "    * `workflow.add_node(generate_answer):` Adds your function that uses the LLM to generate the final answer to the user. This is your robot's `\"Answer Generator\"` skill.\n",
        "   4. `Defining Edges (workflow.add_edge(...) and workflow.add_conditional_edges(...))`\n",
        "    This is like drawing the arrows on your flowchart, defining the sequence of operations:\n",
        "    * `workflow.add_edge(START, \"generate_query_or_respond\"):`\n",
        "\n",
        "      * This is the initial edge. When the graph starts, the very first thing it does is go to the `generate_query_or_respond` node. Your robot always starts by deciding what to do.\n",
        "    * `workflow.add_conditional_edges(\"generate_query_or_respond\", tools_condition, {...}):`\n",
        "       * This is a **conditional transition** from the `generate_query_or_respond` node.\n",
        "       * After `generate_query_or_respond` runs, `LangGraph` looks at its output using `tools_condition`.\n",
        "       * `tools_condition`: This helper checks if the LLM's output (`AIMessage`) contains `tool_calls`.\n",
        "         * If `generate_query_or_respond` decided to use a tool (i.e., its output contains `tool_calls`), `tools_condition` returns the string `\"tools\"`.\n",
        "         * If `generate_query_or_respond` generated a direct answer (no `tool_calls`), `tools_condition` returns END.\n",
        "       * `{\"tools\": \"retrieve\", END: END, }:` This is the mapping.\n",
        "          * If `tools_condition` returns `\"tools\"`, the robot moves to the `\"retrieve\"` node.\n",
        "          * If `tools_condition` returns END, the robot stops immediately (it has answered the user directly).\n",
        "       * **`Robot Logic:`** \"If I need to search for information, go search. Otherwise, if I have the answer, I'm done.\"\n",
        "    * `workflow.add_conditional_edges(\"retrieve\", grade_documents,):`\n",
        "      * This is a conditional transition from the `\"retrieve\"` node (after the tool has executed and its output is in the `state`).\n",
        "      * `grade_documents:` This is the function you defined. Remember, `grade_documents` explicitly returns either `\"generate_answer\"` or `\"rewrite_question\"`.\n",
        "      * `LangGraph` automatically interprets these string returns as the names of the next nodes to go to.\n",
        "      * **Robot Logic:** \"After I've searched, I need to check if the search results are good. If they are, go answer. If not, go refine my question.\"\n",
        "    * `workflow.add_edge(\"generate_answer\", END):`\n",
        "\n",
        "      * This is a direct, unconditional edge.\n",
        "      * After the `generate_answer` node runs (meaning the robot has formulated its final answer to the user), the workflow immediately goes to END.\n",
        "      * **Robot Logic:** \"Once I've answered, I'm done for this turn.\"\n",
        "   * `workflow.add_edge(\"rewrite_question\", \"generate_query_or_respond\"):`\n",
        "\n",
        "     * This is another direct, unconditional edge.\n",
        "     * After the rewrite_question node runs (meaning the robot has rephrased the user's query), the workflow loops back to `generate_query_or_respond`.\n",
        "     * **Robot Logic:** \"If my search wasn't good, I'll rephrase the question and then go back to the beginning to decide whether to search again (with the new question) or if I can answer it now.\" This creates a **self-correction loop**!\n",
        "  5. `graph = workflow.compile()`\n",
        "    * **compile():** This is the final step. It takes your defined workflow (nodes and edges) and compiles it into an executable graph object. It's like taking your robot's blueprint and turning it into a working circuit board. The graph object is now ready to be run.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8R_zpOo7e95q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.prebuilt import tools_condition\n",
        "# 1. Define the state of our graph\n",
        "workflow = StateGraph(MessagesState)\n",
        "# 2. Add the individual \"skills\" or \"steps\" (nodes) to our graph\n",
        "# Define the nodes we will cycle between\n",
        "workflow.add_node(generate_query_or_respond) # Node A: Decides to retrieve or respond\n",
        "workflow.add_node(\"retrieve\", ToolNode([retriever_tool])) # Node B: Executes the retrieval tool\n",
        "workflow.add_node(rewrite_question) # Node C: Rewrites the user's question\n",
        "workflow.add_node(generate_answer) # Node D: Generates the final answer\n",
        "# 3. Define how the robot starts its process\n",
        "workflow.add_edge(START, \"generate_query_or_respond\") # Always start by deciding what to do\n",
        "\n",
        "# 4. Define conditional paths from the decision node\n",
        "# Decide whether to retrieve\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate_query_or_respond\", # From this node...\n",
        "    # Assess LLM decision (call `retriever_tool` tool or respond to the user)\n",
        "    tools_condition,  # ...based on whether it wants to use a tool...\n",
        "    {\n",
        "        # Translate the condition outputs to nodes in our graph\n",
        "        \"tools\": \"retrieve\",  # ...if it wants a tool, go to \"retrieve\" node\n",
        "        END: END,  # ...otherwise (if it gave a direct answer), stop\n",
        "    },\n",
        ")\n",
        "# 5. Define conditional paths after retrieval\n",
        "# Edges taken after the `action` node is called.\n",
        "workflow.add_conditional_edges(\n",
        "    \"retrieve\",\n",
        "    # Assess agent decision\n",
        "    grade_documents,\n",
        ")\n",
        "# 6. Define fixed paths for ending or looping\n",
        "workflow.add_edge(\"generate_answer\", END)\n",
        "workflow.add_edge(\"rewrite_question\", \"generate_query_or_respond\")\n",
        "# 7. Compile the graph (like building the robot's circuit board)\n",
        "# Compile\n",
        "graph = workflow.compile()"
      ],
      "metadata": {
        "id": "frFhvPvY1jW5"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visual Portray\n",
        "In essence, this single line of code generates and displays a **visual flowchart** of your entire RAG agent's workflow directly within your notebook.\n",
        "\n",
        "This is incredibly useful for:\n",
        "\n",
        "  * `Understanding the Flow`: It makes complex conditional logic and loops (like your `rewrite_question` loop) much easier to grasp at a glance.\n",
        "  * `Debugging`: If your agent isn't behaving as expected, looking at the **visual graph** can quickly help you identify missing edges, incorrect conditions, or unexpected paths.\n",
        "  * `Documentation`: It serves as a clear, **self-documenting visual representation** of your agent's architecture.\n",
        "\n",
        "You'll see a diagram showing the `START` node flowing into `generate_query_or_respond`, then branching to `retrieve` or `END`, and further branching from `retrieve` based on `grade_documents` results, potentially looping back with `rewrite_question`. It's your agent's brain, **visually laid** out!\n",
        "\n",
        "This line of code isn't about the logic of your RAG agent, but rather about visualizing the complex workflow you just defined with LangGraph. It helps you see the \"flowchart\" you built.\n",
        "\n",
        "**Understanding the Code**\n",
        "  1. `from IPython.display import Image, display:`\n",
        "\n",
        "    * This imports specific tools from the `IPython.display` module. This module is super handy when you're working in interactive environments like Jupyter notebooks (or Google Colab, VS Code notebooks, etc.).\n",
        "    * `Image:` A class that allows you to display image data (like PNGs, JPEGs) directly within your notebook output.\n",
        "    * `display:` A function that tells the `IPython` environment to render a given object (like an Image object) in the output cell.\n",
        "  2. `graph:`\n",
        "\n",
        "    * This refers to the graph object you created in the previous step using `workflow.compile()`.\n",
        "    * This `graph` object holds the complete, executable definition of your RAG agent's workflow, including all its nodes and edges.\n",
        "  3. `.get_graph():`\n",
        "\n",
        "    * This is a method on the graph object (your compiled `LangGraph` workflow).\n",
        "    * It returns an underlying representation of the graph's structure. Think of it as getting the **raw blueprint** data.\n",
        "  4. `.draw_mermaid_png():`\n",
        "\n",
        "     * This is a method called on the graph's underlying structure (obtained from `.get_graph()`).\n",
        "     * `Mermaid:` This is a popular Markdown-like syntax for generating `diagrams` and `flowcharts`. `LangGraph` can automatically convert its internal graph structure into `Mermaid` syntax.\n",
        "     * `_png():` This part specifies that you want the `Mermaid diagram` to be rendered directly into a PNG image format.\n",
        "     * **What it does:** This method effectively generates the image data (the bytes of a PNG file) that visually represents your `LangGraph` workflow. It translates your `add_node, add_edge, and add_conditional_edges` calls into a visual diagram.\n",
        "  5. `display(Image(...)):`\n",
        "\n",
        "     * Finally, the `Image` class takes the raw PNG image data generated by `.draw_mermaid_png().`\n",
        "     * The `display()` function then renders this Image object directly in the output cell of your notebook.\n"
      ],
      "metadata": {
        "id": "78BOP6xBb2_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "-R6cUFax1lyy",
        "outputId": "80dc5505-90e0-4ea7-97c1-227340a183ae"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Executing the Graph**\n",
        "1. `for chunk in graph.stream(...):`\n",
        "\n",
        "   * `graph`: This is the compiled `LangGraph` object you built (`workflow.compile()`). It's your complete RAG agent's brain.\n",
        "   * `.stream(...):` This is a powerful method to execute your graph. Instead of waiting for the entire process to finish, `stream()` yields `\"chunks\"` of updates as each node in the graph completes its execution. This is incredibly useful for:\n",
        "     * `Seeing progress`: You can watch the agent `\"think\"` step by step.\n",
        "     * `Long-running tasks`: You get feedback incrementally rather than waiting for minutes.\n",
        "     * `Debugging`: You can pinpoint exactly where the flow goes and what each node outputs.\n",
        "2. **Initial Input to the Graph:**\n",
        "```\n",
        "{\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        " * This is the starting `state` you provide to the graph. It's the first message from the user, kicking off the entire process.\n",
        " * You've chosen a question that should trigger your RAG capabilities `(tool use, retrieval, grading, etc.).`\n",
        "\n",
        "3. Outer `for` loop `(for chunk in ...)`:\n",
        "\n",
        "   * As the `graph` executes, for each node that completes its work and potentially updates the graph's `state`, `graph.stream()` yields a chunk.\n",
        "   * A `chunk` is typically a dictionary where the keys are the names of the nodes that just executed, and the values are the specific updates those nodes made to the state.\n",
        "4. Inner `for` loop `(for node, update in chunk.items():)`:\n",
        "  * This `loop` iterates through the items within each chunk. For a linear flow, a chunk often contains just one node and its update.\n",
        "  * `node`: This variable will hold the name of the node that just executed (e.g., `\"generate_query_or_respond\", \"retrieve\", \"generate_answer\")`.\n",
        "  * `update:` This variable holds the actual change to the state made by that node. For nodes that add messages to the conversation history, this update dictionary will look like `{\"messages\": [new_message]}`\n",
        "5. `print(\"Update from node\", node):`\n",
        "  * This simply prints the name of the node that produced the current update, making it easy to follow the flow of execution.\n",
        "6. `if \"messages\" in update and update[\"messages\"]::`\n",
        "\n",
        "   * `Crucial check`: Not every node will necessarily add a new message to the `state[\"messages\"]` list. For example, `grade_documents` returns a string that dictates the next edge, it doesn't add a message. `rewrite_question` does add a message (the rewritten query).\n",
        "   * This `if` condition ensures that you only try to print the last message if the `update` actually contains a messages key and that `list` is not empty.\n",
        "   * `update[\"messages\"][-1].pretty_print():`\n",
        "      * If there's a new message, this accesses the messages list within the `update` dictionary, gets the last (and usually only) new message, and calls its `.pretty_print()` method to display it in a readable format. This lets you see the direct output of each step (e.g., the LLM's decision to call a `tool`, the `tool's` output, the final answer, or a rewritten question).\n",
        "      * `else: print(f\"Node {node} did not add a new message to the state.\")`: This handles cases like `grade_documents` where the node's primary output is controlling the graph flow, not adding a message to the history.\n",
        "\n",
        "7. `print(\"\\n\\n\"):`\n",
        "\n",
        "  * Adds some blank lines for visual separation between updates from different nodes, making the output cleaner.\n"
      ],
      "metadata": {
        "id": "H-R3fl-KgcOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# This loop runs the entire graph workflow\n",
        "for chunk in graph.stream( # 1. Execute the graph in a streaming fashion\n",
        "    {\n",
        "        \"messages\": [ # 2. Initial input (user's first message)\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "):\n",
        "    for node, update in chunk.items(): # 3. Iterate through updates from each node\n",
        "        print(\"Update from node\", node) # 4. Print which node just executed\n",
        "        # 5. Print the last message added to the state by that node (if any)\n",
        "        if \"messages\" in update and update[\"messages\"]: # Check if 'messages' key exists and is not empty\n",
        "            update[\"messages\"][-1].pretty_print()\n",
        "        else:\n",
        "            print(f\"Node {node} did not add a new message to the state.\") # Explain nodes like grade_documents\n",
        "        print(\"\\n\\n\") # For better readability"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7B3RnC-T1oaT",
        "outputId": "07082f6b-4599-4f59-d065-7809a8e90b26"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Update from node generate_query_or_respond\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  retrieve_blog_posts (call_3pQJs0a87KU8Q4JC0r6tEnGl)\n",
            " Call ID: call_3pQJs0a87KU8Q4JC0r6tEnGl\n",
            "  Args:\n",
            "    query: types of reward hacking\n",
            "\n",
            "\n",
            "\n",
            "Update from node retrieve\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: retrieve_blog_posts\n",
            "\n",
            "Detecting Reward Hacking#\n",
            "\n",
            "In-Context Reward Hacking#\n",
            "\n",
            "(Note: Some work defines reward tampering as a distinct category of misalignment behavior from reward hacking. But I consider reward hacking as a broader concept here.)\n",
            "At a high level, reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering.\n",
            "\n",
            "Why does Reward Hacking Exist?#\n",
            "\n",
            "\n",
            "\n",
            "Update from node generate_answer\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Lilian Weng categorizes reward hacking into two types: environment or goal misspecification, and reward tampering.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}