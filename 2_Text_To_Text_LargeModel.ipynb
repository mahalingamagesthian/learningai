{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4735f99b-6b4e-4acb-ae39-de44817c65db",
   "metadata": {},
   "source": [
    "Let's take the text-to-text generation concept to the next stage by integrating a more powerful Large Language Model (LLM) from Hugging Face. We'll use a model that is known for its strong generative capabilities. For this demonstration, we'll use google/flan-t5-large. This is a larger version of the T5 model, fine-tuned on a diverse set of tasks using instruction tuning, which often leads to better zero-shot and few-shot performance. Here's the updated code that you can run in your JupyterHub environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3cba62d-68be-4c09-bf22-9f6a62ba8536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# Specify a more powerful LLM for text-to-text generation\n",
    "model_name = \"google/flan-t5-large\"\n",
    "\n",
    "# Create a text-to-text generation pipeline with the LLM\n",
    "generator = pipeline(\"text2text-generation\", model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f0211-1684-4012-aead-5c7f7dc23d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Open-ended generation (you can provide a starting prompt)\n",
    "prompt_1 = \"Write a short story about a robot who dreams of becoming a painter.\"\n",
    "output_1 = generator(prompt_1, max_length=200, num_return_sequences=1)\n",
    "print(f\"Prompt 1: {prompt_1}\")\n",
    "print(f\"Output 1: {output_1[0]['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a46f529-b27b-4f8e-9f43-e561b413e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Creative writing with a specific style\n",
    "prompt_2 = \"Write a poem in the style of Robert Frost about a walk in the woods.\"\n",
    "output_2 = generator(prompt_2, max_length=150, num_return_sequences=1)\n",
    "print(f\"Prompt 2: {prompt_2}\")\n",
    "print(f\"Output 2: {output_2[0]['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacb8b34-f54e-4374-a2a4-690361d73d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Answering a question with more context (though Flan-T5 is primarily text-to-text)\n",
    "prompt_3 = \"Answer the question: What are the main benefits of using foundation models?\"\n",
    "output_3 = generator(prompt_3, max_length=150, num_return_sequences=1)\n",
    "print(f\"Prompt 3: {prompt_3}\")\n",
    "print(f\"Output 3: {output_3[0]['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d803e567-2d81-4466-ae9c-89a9c2980ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Summarization (while there are dedicated summarization models, LLMs can often do this)\n",
    "long_text = \"\"\"\n",
    "The concept of foundation models has revolutionized the field of artificial intelligence. Trained on vast amounts of unlabeled data, these models learn general-purpose representations that can be fine-tuned for a wide variety of downstream tasks. This paradigm shift has moved away from training specialized models for each specific problem, offering significant advantages in terms of data efficiency and model development. Foundation models like large language models (LLMs) have demonstrated impressive capabilities in natural language processing, including text generation, translation, and question answering. Similarly, foundation models in computer vision have advanced tasks such as image recognition and object detection. The ability of these models to adapt to new tasks with minimal task-specific data has accelerated progress across different AI domains and opened up new possibilities for real-world applications.\n",
    "\"\"\"\n",
    "prompt_4 = f\"Summarize the following text: {long_text}\"\n",
    "output_4 = generator(prompt_4, max_length=100, num_return_sequences=1)\n",
    "print(f\"Prompt 4: {prompt_4}\")\n",
    "print(f\"Output 4: {output_4[0]['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3100945d-52a9-4d47-bf98-66154f7371c4",
   "metadata": {},
   "source": [
    "# Explanation: #\n",
    "\n",
    "`model_name = \"google/flan-t5-large\"`: We've switched the model_name to \"google/flan-t5-large\". This is a significantly larger and more capable LLM compared to t5-small.\n",
    "\n",
    "`More Open-Ended Generation`: The first example demonstrates more open-ended text generation based on a creative prompt. LLMs excel at this type of task.\n",
    "\n",
    "`Style Transfer (Poetry)`: The second example attempts to generate text in a specific style, showcasing the model's understanding of language nuances.\n",
    "\n",
    "`Question Answering (Direct Prompt)`: While Flan-T5 is a text-to-text model, it can often handle question answering when the question is directly phrased. Dedicated question-answering pipelines might be more specialized, but LLMs show general understanding.\n",
    "\n",
    "`Summarization`: The fourth example demonstrates using the LLM for summarization, a common application of text generation models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cea885-ab6a-4660-a83b-00ab8eb1d54b",
   "metadata": {},
   "source": [
    "# Running the Code #\n",
    "\n",
    "Ensure you have the transformers and torch libraries installed in your JupyterHub environment (as mentioned in the previous response). Copy and paste this new code into a Jupyter notebook cell. Run the cell. The first time you run it with `google/flan-t5-large`, it will take longer to download the model weights because it's a much larger model than t5-small. Subsequent runs will be faster due to caching.\n",
    "\n",
    "# Important Considerations: #\n",
    "\n",
    "`Computational Resources`: Running larger LLMs like flan-t5-large requires more computational resources (CPU, RAM, and potentially GPU if available and configured). Be mindful of the resources available in your JupyterHub environment. If you encounter memory issues, you might need to use a smaller model or optimize your environment. `Inference Time`: Generating text with larger models will generally take longer than with smaller models. `Output Quality`: You should observe a potential improvement in the quality, coherence, and creativity of the generated text compared to t5-small. However, the output is not always perfect and can sometimes be nonsensical or repetitive. `Prompt Engineering`: The quality of the output from an LLM heavily depends on the prompt you provide. Experiment with different phrasings and instructions to guide the model towards the desired output. This step demonstrates how you can leverage more powerful foundation models for more complex and nuanced text generation tasks within the Hugging Face ecosystem in your JupyterHub environment. You can continue to explore even larger and more specialized LLMs available on the Hugging Face Model Hub based on your specific needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd113034-0371-44ee-9047-92e33bd91b52",
   "metadata": {},
   "source": [
    "# Model Location # \n",
    "\n",
    "The model gets downloaded and stored in the following locations:\n",
    "`.cache/huggingface/hub`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f94b6ec-d6af-40c8-ab3d-63a20f59045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identiying the location of the cache directory \n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "cache_dir = api.cache_dir\n",
    "print(f\"Hugging Face cache directory: {cache_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d820a4-c30f-4629-b32f-8d380b8ca422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#redirecting the model to a new location\n",
    "import os\n",
    "os.environ['HF_HUB_CACHE'] = '/path/to/your/desired/cache/location'\n",
    "\n",
    "from transformers import StableDiffusionPipeline\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(model_id) \n",
    "# Model will be downloaded to the new cache location"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
